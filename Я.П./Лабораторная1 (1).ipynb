{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995d0dc9-320e-4a53-a892-3a7500bf7cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\dasha\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\dasha\\anaconda3\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\dasha\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dasha\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02dad173-2145-4712-927c-b0300d6e041a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "Layer                          Output Shape              Param #        \n",
      "======================================================================\n",
      "bert.embeddings.word_embeddings.weight torch.Size([30522, 768])  23440896       \n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768])    393216         \n",
      "bert.embeddings.token_type_embeddings.weight torch.Size([2, 768])      1536           \n",
      "bert.embeddings.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.embeddings.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])   2359296        \n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])        3072           \n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])   2359296        \n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])   2359296        \n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])        3072           \n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])   2359296        \n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])   2359296        \n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])        3072           \n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])   2359296        \n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])   2359296        \n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])        3072           \n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])   2359296        \n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])   2359296        \n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])        3072           \n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])   2359296        \n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])   2359296        \n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])        3072           \n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])   2359296        \n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])   2359296        \n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])        3072           \n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])   2359296        \n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])   2359296        \n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])        3072           \n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])   2359296        \n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])   2359296        \n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])        3072           \n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])   2359296        \n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])   2359296        \n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])        3072           \n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])   2359296        \n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])   2359296        \n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])        3072           \n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])   2359296        \n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])    589824         \n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])   2359296        \n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])        3072           \n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])   2359296        \n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])         768            \n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])         768            \n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])         768            \n",
      "bert.pooler.dense.weight       torch.Size([768, 768])    589824         \n",
      "bert.pooler.dense.bias         torch.Size([768])         768            \n",
      "fc.weight                      torch.Size([1, 768])      768            \n",
      "fc.bias                        torch.Size([1])           1              \n",
      "======================================================================\n",
      "Total params: 109483009\n",
      "1/1094 ━━━━━━━━━━━━━━━━━━━━ 9.6s - loss: 0.5702\n",
      "2/1094 ━━━━━━━━━━━━━━━━━━━━ 18.2s - loss: 0.4950\n",
      "3/1094 ━━━━━━━━━━━━━━━━━━━━ 26.2s - loss: 0.4548\n",
      "4/1094 ━━━━━━━━━━━━━━━━━━━━ 34.0s - loss: 0.4345\n",
      "5/1094 ━━━━━━━━━━━━━━━━━━━━ 43.1s - loss: 0.4145\n",
      "6/1094 ━━━━━━━━━━━━━━━━━━━━ 54.0s - loss: 0.3966\n",
      "7/1094 ━━━━━━━━━━━━━━━━━━━━ 62.1s - loss: 0.3722\n",
      "8/1094 ━━━━━━━━━━━━━━━━━━━━ 70.7s - loss: 0.3441\n",
      "9/1094 ━━━━━━━━━━━━━━━━━━━━ 79.5s - loss: 0.3046\n",
      "10/1094 ━━━━━━━━━━━━━━━━━━━━ 87.9s - loss: 0.2831\n",
      "11/1094 ━━━━━━━━━━━━━━━━━━━━ 97.7s - loss: 0.2634\n",
      "12/1094 ━━━━━━━━━━━━━━━━━━━━ 108.6s - loss: 0.2510\n",
      "13/1094 ━━━━━━━━━━━━━━━━━━━━ 119.2s - loss: 0.2280\n",
      "14/1094 ━━━━━━━━━━━━━━━━━━━━ 129.1s - loss: 0.2041\n",
      "15/1094 ━━━━━━━━━━━━━━━━━━━━ 138.5s - loss: 0.1865\n",
      "16/1094 ━━━━━━━━━━━━━━━━━━━━ 148.7s - loss: 0.1721\n",
      "17/1094 ━━━━━━━━━━━━━━━━━━━━ 157.7s - loss: 0.1611\n",
      "18/1094 ━━━━━━━━━━━━━━━━━━━━ 166.5s - loss: 0.1509\n",
      "19/1094 ━━━━━━━━━━━━━━━━━━━━ 175.4s - loss: 0.1389\n",
      "20/1094 ━━━━━━━━━━━━━━━━━━━━ 184.2s - loss: 0.1285\n",
      "21/1094 ━━━━━━━━━━━━━━━━━━━━ 193.2s - loss: 0.1179\n",
      "22/1094 ━━━━━━━━━━━━━━━━━━━━ 202.2s - loss: 0.1113\n",
      "23/1094 ━━━━━━━━━━━━━━━━━━━━ 211.2s - loss: 0.1052\n",
      "24/1094 ━━━━━━━━━━━━━━━━━━━━ 220.2s - loss: 0.0978\n",
      "25/1094 ━━━━━━━━━━━━━━━━━━━━ 229.1s - loss: 0.0924\n",
      "26/1094 ━━━━━━━━━━━━━━━━━━━━ 238.6s - loss: 0.0852\n",
      "27/1094 ━━━━━━━━━━━━━━━━━━━━ 246.9s - loss: 0.0816\n",
      "28/1094 ━━━━━━━━━━━━━━━━━━━━ 255.2s - loss: 0.0769\n",
      "29/1094 ━━━━━━━━━━━━━━━━━━━━ 264.1s - loss: 0.0742\n",
      "30/1094 ━━━━━━━━━━━━━━━━━━━━ 273.5s - loss: 0.0701\n",
      "31/1094 ━━━━━━━━━━━━━━━━━━━━ 283.9s - loss: 0.0667\n",
      "32/1094 ━━━━━━━━━━━━━━━━━━━━ 293.3s - loss: 0.0632\n",
      "33/1094 ━━━━━━━━━━━━━━━━━━━━ 302.7s - loss: 0.0588\n",
      "34/1094 ━━━━━━━━━━━━━━━━━━━━ 313.0s - loss: 0.0569\n",
      "35/1094 ━━━━━━━━━━━━━━━━━━━━ 322.2s - loss: 0.0547\n",
      "36/1094 ━━━━━━━━━━━━━━━━━━━━ 331.2s - loss: 0.0510\n",
      "37/1094 ━━━━━━━━━━━━━━━━━━━━ 340.6s - loss: 0.0476\n",
      "38/1094 ━━━━━━━━━━━━━━━━━━━━ 349.5s - loss: 0.0457\n",
      "39/1094 ━━━━━━━━━━━━━━━━━━━━ 357.6s - loss: 0.0434\n",
      "40/1094 ━━━━━━━━━━━━━━━━━━━━ 365.1s - loss: 0.0419\n",
      "41/1094 ━━━━━━━━━━━━━━━━━━━━ 373.5s - loss: 0.0397\n",
      "42/1094 ━━━━━━━━━━━━━━━━━━━━ 382.2s - loss: 0.0382\n",
      "43/1094 ━━━━━━━━━━━━━━━━━━━━ 390.6s - loss: 0.0366\n",
      "44/1094 ━━━━━━━━━━━━━━━━━━━━ 400.4s - loss: 0.0349\n",
      "45/1094 ━━━━━━━━━━━━━━━━━━━━ 409.7s - loss: 0.0332\n",
      "46/1094 ━━━━━━━━━━━━━━━━━━━━ 420.0s - loss: 0.0319\n",
      "47/1094 ━━━━━━━━━━━━━━━━━━━━ 429.4s - loss: 0.0307\n",
      "48/1094 ━━━━━━━━━━━━━━━━━━━━ 438.7s - loss: 0.0297\n",
      "49/1094 ━━━━━━━━━━━━━━━━━━━━ 448.4s - loss: 0.0290\n",
      "50/1094 ━━━━━━━━━━━━━━━━━━━━ 457.0s - loss: 0.0273\n",
      "51/1094 ━━━━━━━━━━━━━━━━━━━━ 466.9s - loss: 0.0265\n",
      "52/1094 ━━━━━━━━━━━━━━━━━━━━ 477.1s - loss: 0.0256\n",
      "53/1094 ━━━━━━━━━━━━━━━━━━━━ 486.1s - loss: 0.0248\n",
      "54/1094 ━━━━━━━━━━━━━━━━━━━━ 495.6s - loss: 0.0240\n",
      "55/1094 ━━━━━━━━━━━━━━━━━━━━ 505.3s - loss: 0.0231\n",
      "56/1094 ━━━━━━━━━━━━━━━━━━━━ 513.7s - loss: 0.0229\n",
      "57/1094 ━━━━━━━━━━━━━━━━━━━━ 521.7s - loss: 0.0220\n",
      "58/1094 ━━━━━━━━━━━━━━━━━━━━ 532.7s - loss: 0.0210\n",
      "59/1094 ━━━━━━━━━━━━━━━━━━━━ 541.3s - loss: 0.0204\n",
      "60/1094 ━━━━━━━━━━━━━━━━━━━━ 550.5s - loss: 0.0202\n",
      "61/1094 ━━━━━━━━━━━━━━━━━━━━ 560.1s - loss: 0.0196\n",
      "62/1094 ━━━━━━━━━━━━━━━━━━━━ 570.8s - loss: 0.0191\n",
      "63/1094 ━━━━━━━━━━━━━━━━━━━━ 580.9s - loss: 0.0183\n",
      "64/1094 ━━━━━━━━━━━━━━━━━━━━ 591.2s - loss: 0.0182\n",
      "65/1094 ━━━━━━━━━━━━━━━━━━━━ 600.5s - loss: 0.0178\n",
      "66/1094 ━━━━━━━━━━━━━━━━━━━━ 610.1s - loss: 0.0172\n",
      "67/1094 ━━━━━━━━━━━━━━━━━━━━ 618.9s - loss: 0.0166\n",
      "68/1094 ━━━━━━━━━━━━━━━━━━━━ 628.0s - loss: 0.0162\n",
      "69/1094 ━━━━━━━━━━━━━━━━━━━━ 637.4s - loss: 0.0161\n",
      "70/1094 ━━━━━━━━━━━━━━━━━━━━ 645.8s - loss: 0.0158\n",
      "71/1094 ━━━━━━━━━━━━━━━━━━━━ 653.7s - loss: 0.0153\n",
      "72/1094 ━━━━━━━━━━━━━━━━━━━━ 661.8s - loss: 0.0153\n",
      "73/1094 ━━━━━━━━━━━━━━━━━━━━ 670.6s - loss: 0.0149\n",
      "74/1094 ━━━━━━━━━━━━━━━━━━━━ 679.6s - loss: 0.0144\n",
      "75/1094 ━━━━━━━━━━━━━━━━━━━━ 688.8s - loss: 0.0142\n",
      "76/1094 ━━━━━━━━━━━━━━━━━━━━ 697.5s - loss: 0.0138\n",
      "77/1094 ━━━━━━━━━━━━━━━━━━━━ 706.4s - loss: 0.0136\n",
      "78/1094 ━━━━━━━━━━━━━━━━━━━━ 715.5s - loss: 0.0134\n",
      "79/1094 ━━━━━━━━━━━━━━━━━━━━ 724.7s - loss: 0.0131\n",
      "80/1094 ━━━━━━━━━━━━━━━━━━━━ 733.7s - loss: 0.0128\n",
      "81/1094 ━━━━━━━━━━━━━━━━━━━━ 743.3s - loss: 0.0126\n",
      "82/1094 ━━━━━━━━━━━━━━━━━━━━ 752.8s - loss: 0.0124\n",
      "83/1094 ━━━━━━━━━━━━━━━━━━━━ 761.2s - loss: 0.0120\n",
      "84/1094 ━━━━━━━━━━━━━━━━━━━━ 771.0s - loss: 0.0123\n",
      "85/1094 ━━━━━━━━━━━━━━━━━━━━ 779.2s - loss: 0.0117\n",
      "86/1094 ━━━━━━━━━━━━━━━━━━━━ 787.5s - loss: 0.0118\n",
      "87/1094 ━━━━━━━━━━━━━━━━━━━━ 795.8s - loss: 0.0115\n",
      "88/1094 ━━━━━━━━━━━━━━━━━━━━ 806.0s - loss: 0.0111\n",
      "89/1094 ━━━━━━━━━━━━━━━━━━━━ 815.6s - loss: 0.0110\n",
      "90/1094 ━━━━━━━━━━━━━━━━━━━━ 824.7s - loss: 0.0107\n",
      "91/1094 ━━━━━━━━━━━━━━━━━━━━ 834.0s - loss: 0.0108\n",
      "92/1094 ━━━━━━━━━━━━━━━━━━━━ 842.3s - loss: 0.0105\n",
      "93/1094 ━━━━━━━━━━━━━━━━━━━━ 853.3s - loss: 0.0104\n",
      "94/1094 ━━━━━━━━━━━━━━━━━━━━ 861.8s - loss: 0.0103\n",
      "95/1094 ━━━━━━━━━━━━━━━━━━━━ 870.8s - loss: 0.0101\n",
      "96/1094 ━━━━━━━━━━━━━━━━━━━━ 880.5s - loss: 0.0101\n",
      "97/1094 ━━━━━━━━━━━━━━━━━━━━ 889.1s - loss: 0.0097\n",
      "98/1094 ━━━━━━━━━━━━━━━━━━━━ 898.3s - loss: 0.0096\n",
      "99/1094 ━━━━━━━━━━━━━━━━━━━━ 907.5s - loss: 0.0095\n",
      "100/1094 ━━━━━━━━━━━━━━━━━━━━ 915.9s - loss: 0.0093\n",
      "101/1094 ━━━━━━━━━━━━━━━━━━━━ 924.1s - loss: 0.0091\n",
      "102/1094 ━━━━━━━━━━━━━━━━━━━━ 932.4s - loss: 0.0091\n",
      "103/1094 ━━━━━━━━━━━━━━━━━━━━ 943.2s - loss: 0.0089\n",
      "104/1094 ━━━━━━━━━━━━━━━━━━━━ 951.1s - loss: 0.0088\n",
      "105/1094 ━━━━━━━━━━━━━━━━━━━━ 960.0s - loss: 0.0087\n",
      "106/1094 ━━━━━━━━━━━━━━━━━━━━ 967.7s - loss: 0.0085\n",
      "107/1094 ━━━━━━━━━━━━━━━━━━━━ 975.8s - loss: 0.0084\n",
      "108/1094 ━━━━━━━━━━━━━━━━━━━━ 984.4s - loss: 0.0084\n",
      "109/1094 ━━━━━━━━━━━━━━━━━━━━ 992.6s - loss: 0.0081\n",
      "110/1094 ━━━━━━━━━━━━━━━━━━━━ 1000.5s - loss: 0.0080\n",
      "111/1094 ━━━━━━━━━━━━━━━━━━━━ 1009.0s - loss: 0.0080\n",
      "112/1094 ━━━━━━━━━━━━━━━━━━━━ 1017.2s - loss: 0.0080\n",
      "113/1094 ━━━━━━━━━━━━━━━━━━━━ 1027.1s - loss: 0.0077\n",
      "114/1094 ━━━━━━━━━━━━━━━━━━━━ 1036.3s - loss: 0.0077\n",
      "115/1094 ━━━━━━━━━━━━━━━━━━━━ 1044.6s - loss: 0.0076\n",
      "116/1094 ━━━━━━━━━━━━━━━━━━━━ 1052.8s - loss: 0.0074\n",
      "117/1094 ━━━━━━━━━━━━━━━━━━━━ 1062.5s - loss: 0.0074\n",
      "118/1094 ━━━━━━━━━━━━━━━━━━━━ 1072.3s - loss: 0.0074\n",
      "119/1094 ━━━━━━━━━━━━━━━━━━━━ 1081.2s - loss: 0.0072\n",
      "120/1094 ━━━━━━━━━━━━━━━━━━━━ 1090.6s - loss: 0.0071\n",
      "121/1094 ━━━━━━━━━━━━━━━━━━━━ 1099.8s - loss: 0.0071\n",
      "122/1094 ━━━━━━━━━━━━━━━━━━━━ 1109.2s - loss: 0.0069\n",
      "123/1094 ━━━━━━━━━━━━━━━━━━━━ 1118.9s - loss: 0.0068\n",
      "124/1094 ━━━━━━━━━━━━━━━━━━━━ 1128.3s - loss: 0.0069\n",
      "125/1094 ━━━━━━━━━━━━━━━━━━━━ 1137.3s - loss: 0.0066\n",
      "126/1094 ━━━━━━━━━━━━━━━━━━━━ 1146.9s - loss: 0.0065\n",
      "127/1094 ━━━━━━━━━━━━━━━━━━━━ 1154.9s - loss: 0.0065\n",
      "128/1094 ━━━━━━━━━━━━━━━━━━━━ 1163.4s - loss: 0.0065\n",
      "129/1094 ━━━━━━━━━━━━━━━━━━━━ 1172.7s - loss: 0.0063\n",
      "130/1094 ━━━━━━━━━━━━━━━━━━━━ 1180.4s - loss: 0.0063\n",
      "131/1094 ━━━━━━━━━━━━━━━━━━━━ 1189.0s - loss: 0.0062\n",
      "132/1094 ━━━━━━━━━━━━━━━━━━━━ 1197.0s - loss: 0.0062\n",
      "133/1094 ━━━━━━━━━━━━━━━━━━━━ 1207.6s - loss: 0.0061\n",
      "134/1094 ━━━━━━━━━━━━━━━━━━━━ 1217.5s - loss: 0.0060\n",
      "135/1094 ━━━━━━━━━━━━━━━━━━━━ 1227.9s - loss: 0.0059\n",
      "136/1094 ━━━━━━━━━━━━━━━━━━━━ 1237.3s - loss: 0.0059\n",
      "137/1094 ━━━━━━━━━━━━━━━━━━━━ 1246.2s - loss: 0.0058\n",
      "138/1094 ━━━━━━━━━━━━━━━━━━━━ 1255.1s - loss: 0.0058\n",
      "139/1094 ━━━━━━━━━━━━━━━━━━━━ 1264.6s - loss: 0.0056\n",
      "140/1094 ━━━━━━━━━━━━━━━━━━━━ 1274.9s - loss: 0.0057\n",
      "141/1094 ━━━━━━━━━━━━━━━━━━━━ 1284.7s - loss: 0.0056\n",
      "142/1094 ━━━━━━━━━━━━━━━━━━━━ 1293.4s - loss: 0.0056\n",
      "143/1094 ━━━━━━━━━━━━━━━━━━━━ 1302.3s - loss: 0.0055\n",
      "144/1094 ━━━━━━━━━━━━━━━━━━━━ 1310.1s - loss: 0.0054\n",
      "145/1094 ━━━━━━━━━━━━━━━━━━━━ 1318.7s - loss: 0.0054\n",
      "146/1094 ━━━━━━━━━━━━━━━━━━━━ 1327.7s - loss: 0.0053\n",
      "147/1094 ━━━━━━━━━━━━━━━━━━━━ 1336.1s - loss: 0.0053\n",
      "148/1094 ━━━━━━━━━━━━━━━━━━━━ 1345.2s - loss: 0.0052\n",
      "149/1094 ━━━━━━━━━━━━━━━━━━━━ 1353.9s - loss: 0.0052\n",
      "150/1094 ━━━━━━━━━━━━━━━━━━━━ 1361.7s - loss: 0.0050\n",
      "151/1094 ━━━━━━━━━━━━━━━━━━━━ 1369.7s - loss: 0.0051\n",
      "152/1094 ━━━━━━━━━━━━━━━━━━━━ 1379.8s - loss: 0.0050\n",
      "153/1094 ━━━━━━━━━━━━━━━━━━━━ 1389.3s - loss: 0.0049\n",
      "154/1094 ━━━━━━━━━━━━━━━━━━━━ 1401.4s - loss: 0.0049\n",
      "155/1094 ━━━━━━━━━━━━━━━━━━━━ 1411.7s - loss: 0.0050\n",
      "156/1094 ━━━━━━━━━━━━━━━━━━━━ 1421.3s - loss: 0.0048\n",
      "157/1094 ━━━━━━━━━━━━━━━━━━━━ 1431.5s - loss: 0.0048\n",
      "158/1094 ━━━━━━━━━━━━━━━━━━━━ 1440.7s - loss: 0.0047\n",
      "159/1094 ━━━━━━━━━━━━━━━━━━━━ 1450.8s - loss: 0.0046\n",
      "160/1094 ━━━━━━━━━━━━━━━━━━━━ 1459.5s - loss: 0.0046\n",
      "161/1094 ━━━━━━━━━━━━━━━━━━━━ 1467.5s - loss: 0.0046\n",
      "162/1094 ━━━━━━━━━━━━━━━━━━━━ 1475.5s - loss: 0.0046\n",
      "163/1094 ━━━━━━━━━━━━━━━━━━━━ 1482.8s - loss: 0.0046\n",
      "164/1094 ━━━━━━━━━━━━━━━━━━━━ 1492.6s - loss: 0.0045\n",
      "165/1094 ━━━━━━━━━━━━━━━━━━━━ 1501.3s - loss: 0.0045\n",
      "166/1094 ━━━━━━━━━━━━━━━━━━━━ 1509.1s - loss: 0.0044\n",
      "167/1094 ━━━━━━━━━━━━━━━━━━━━ 1517.2s - loss: 0.0043\n",
      "168/1094 ━━━━━━━━━━━━━━━━━━━━ 1525.7s - loss: 0.0044\n",
      "169/1094 ━━━━━━━━━━━━━━━━━━━━ 1535.2s - loss: 0.0044\n",
      "170/1094 ━━━━━━━━━━━━━━━━━━━━ 1543.5s - loss: 0.0042\n",
      "171/1094 ━━━━━━━━━━━━━━━━━━━━ 1552.4s - loss: 0.0043\n",
      "172/1094 ━━━━━━━━━━━━━━━━━━━━ 1560.8s - loss: 0.0042\n",
      "173/1094 ━━━━━━━━━━━━━━━━━━━━ 1572.2s - loss: 0.0042\n",
      "174/1094 ━━━━━━━━━━━━━━━━━━━━ 1581.7s - loss: 0.0041\n",
      "175/1094 ━━━━━━━━━━━━━━━━━━━━ 1592.5s - loss: 0.0042\n",
      "176/1094 ━━━━━━━━━━━━━━━━━━━━ 1602.4s - loss: 0.0040\n",
      "177/1094 ━━━━━━━━━━━━━━━━━━━━ 1610.9s - loss: 0.0041\n",
      "178/1094 ━━━━━━━━━━━━━━━━━━━━ 1620.6s - loss: 0.0040\n",
      "179/1094 ━━━━━━━━━━━━━━━━━━━━ 1630.1s - loss: 0.0039\n",
      "180/1094 ━━━━━━━━━━━━━━━━━━━━ 1638.4s - loss: 0.0040\n",
      "181/1094 ━━━━━━━━━━━━━━━━━━━━ 1647.4s - loss: 0.0039\n",
      "182/1094 ━━━━━━━━━━━━━━━━━━━━ 1655.8s - loss: 0.0039\n",
      "183/1094 ━━━━━━━━━━━━━━━━━━━━ 1666.8s - loss: 0.0038\n",
      "184/1094 ━━━━━━━━━━━━━━━━━━━━ 1675.3s - loss: 0.0038\n",
      "185/1094 ━━━━━━━━━━━━━━━━━━━━ 1685.2s - loss: 0.0037\n",
      "186/1094 ━━━━━━━━━━━━━━━━━━━━ 1693.9s - loss: 0.0038\n",
      "187/1094 ━━━━━━━━━━━━━━━━━━━━ 1702.6s - loss: 0.0037\n",
      "188/1094 ━━━━━━━━━━━━━━━━━━━━ 1710.9s - loss: 0.0037\n",
      "189/1094 ━━━━━━━━━━━━━━━━━━━━ 1720.9s - loss: 0.0036\n",
      "190/1094 ━━━━━━━━━━━━━━━━━━━━ 1732.2s - loss: 0.0036\n",
      "191/1094 ━━━━━━━━━━━━━━━━━━━━ 1743.8s - loss: 0.0036\n",
      "192/1094 ━━━━━━━━━━━━━━━━━━━━ 1753.8s - loss: 0.0036\n",
      "193/1094 ━━━━━━━━━━━━━━━━━━━━ 1764.3s - loss: 0.0035\n",
      "194/1094 ━━━━━━━━━━━━━━━━━━━━ 1775.2s - loss: 0.0035\n",
      "195/1094 ━━━━━━━━━━━━━━━━━━━━ 1784.4s - loss: 0.0035\n",
      "196/1094 ━━━━━━━━━━━━━━━━━━━━ 1795.3s - loss: 0.0035\n",
      "197/1094 ━━━━━━━━━━━━━━━━━━━━ 1804.9s - loss: 0.0034\n",
      "198/1094 ━━━━━━━━━━━━━━━━━━━━ 1813.6s - loss: 0.0035\n",
      "199/1094 ━━━━━━━━━━━━━━━━━━━━ 1822.6s - loss: 0.0034\n",
      "200/1094 ━━━━━━━━━━━━━━━━━━━━ 1831.4s - loss: 0.0033\n",
      "201/1094 ━━━━━━━━━━━━━━━━━━━━ 1839.9s - loss: 0.0034\n",
      "202/1094 ━━━━━━━━━━━━━━━━━━━━ 1849.5s - loss: 0.0034\n",
      "203/1094 ━━━━━━━━━━━━━━━━━━━━ 1857.6s - loss: 0.0033\n",
      "204/1094 ━━━━━━━━━━━━━━━━━━━━ 1867.4s - loss: 0.0033\n",
      "205/1094 ━━━━━━━━━━━━━━━━━━━━ 1877.2s - loss: 0.0033\n",
      "206/1094 ━━━━━━━━━━━━━━━━━━━━ 1887.8s - loss: 0.0032\n",
      "207/1094 ━━━━━━━━━━━━━━━━━━━━ 1896.8s - loss: 0.0033\n",
      "208/1094 ━━━━━━━━━━━━━━━━━━━━ 1906.0s - loss: 0.0032\n",
      "209/1094 ━━━━━━━━━━━━━━━━━━━━ 1914.9s - loss: 0.0032\n",
      "210/1094 ━━━━━━━━━━━━━━━━━━━━ 1925.6s - loss: 0.0032\n",
      "211/1094 ━━━━━━━━━━━━━━━━━━━━ 1935.6s - loss: 0.0032\n",
      "212/1094 ━━━━━━━━━━━━━━━━━━━━ 1943.9s - loss: 0.0030\n",
      "213/1094 ━━━━━━━━━━━━━━━━━━━━ 1952.7s - loss: 0.0031\n",
      "214/1094 ━━━━━━━━━━━━━━━━━━━━ 1961.3s - loss: 0.0031\n",
      "215/1094 ━━━━━━━━━━━━━━━━━━━━ 1969.9s - loss: 0.0031\n",
      "216/1094 ━━━━━━━━━━━━━━━━━━━━ 1977.6s - loss: 0.0030\n",
      "217/1094 ━━━━━━━━━━━━━━━━━━━━ 1986.2s - loss: 0.0030\n",
      "218/1094 ━━━━━━━━━━━━━━━━━━━━ 1994.9s - loss: 0.0030\n",
      "219/1094 ━━━━━━━━━━━━━━━━━━━━ 2003.1s - loss: 0.0030\n",
      "220/1094 ━━━━━━━━━━━━━━━━━━━━ 2012.6s - loss: 0.0030\n",
      "221/1094 ━━━━━━━━━━━━━━━━━━━━ 2022.7s - loss: 0.0029\n",
      "222/1094 ━━━━━━━━━━━━━━━━━━━━ 2031.3s - loss: 0.0030\n",
      "223/1094 ━━━━━━━━━━━━━━━━━━━━ 2042.0s - loss: 0.0029\n",
      "224/1094 ━━━━━━━━━━━━━━━━━━━━ 2051.7s - loss: 0.0029\n",
      "225/1094 ━━━━━━━━━━━━━━━━━━━━ 2060.6s - loss: 0.0029\n",
      "226/1094 ━━━━━━━━━━━━━━━━━━━━ 2071.0s - loss: 0.0029\n",
      "227/1094 ━━━━━━━━━━━━━━━━━━━━ 2080.3s - loss: 0.0028\n",
      "228/1094 ━━━━━━━━━━━━━━━━━━━━ 2090.1s - loss: 0.0028\n",
      "229/1094 ━━━━━━━━━━━━━━━━━━━━ 2099.7s - loss: 0.0028\n",
      "230/1094 ━━━━━━━━━━━━━━━━━━━━ 2107.7s - loss: 0.0029\n",
      "231/1094 ━━━━━━━━━━━━━━━━━━━━ 2117.2s - loss: 0.0027\n",
      "232/1094 ━━━━━━━━━━━━━━━━━━━━ 2126.5s - loss: 0.0028\n",
      "233/1094 ━━━━━━━━━━━━━━━━━━━━ 2136.1s - loss: 0.0028\n",
      "234/1094 ━━━━━━━━━━━━━━━━━━━━ 2145.0s - loss: 0.0027\n",
      "235/1094 ━━━━━━━━━━━━━━━━━━━━ 2152.7s - loss: 0.0027\n",
      "236/1094 ━━━━━━━━━━━━━━━━━━━━ 2160.7s - loss: 0.0027\n",
      "237/1094 ━━━━━━━━━━━━━━━━━━━━ 2169.4s - loss: 0.0027\n",
      "238/1094 ━━━━━━━━━━━━━━━━━━━━ 2179.7s - loss: 0.0027\n",
      "239/1094 ━━━━━━━━━━━━━━━━━━━━ 2190.6s - loss: 0.0027\n",
      "240/1094 ━━━━━━━━━━━━━━━━━━━━ 2201.1s - loss: 0.0026\n",
      "241/1094 ━━━━━━━━━━━━━━━━━━━━ 2211.5s - loss: 0.0027\n",
      "242/1094 ━━━━━━━━━━━━━━━━━━━━ 2220.8s - loss: 0.0026\n",
      "243/1094 ━━━━━━━━━━━━━━━━━━━━ 2230.4s - loss: 0.0026\n",
      "244/1094 ━━━━━━━━━━━━━━━━━━━━ 2240.4s - loss: 0.0026\n",
      "245/1094 ━━━━━━━━━━━━━━━━━━━━ 2250.0s - loss: 0.0026\n",
      "246/1094 ━━━━━━━━━━━━━━━━━━━━ 2260.0s - loss: 0.0026\n",
      "247/1094 ━━━━━━━━━━━━━━━━━━━━ 2268.6s - loss: 0.0026\n",
      "248/1094 ━━━━━━━━━━━━━━━━━━━━ 2277.7s - loss: 0.0025\n",
      "249/1094 ━━━━━━━━━━━━━━━━━━━━ 2286.8s - loss: 0.0025\n",
      "250/1094 ━━━━━━━━━━━━━━━━━━━━ 2295.2s - loss: 0.0025\n",
      "251/1094 ━━━━━━━━━━━━━━━━━━━━ 2303.6s - loss: 0.0025\n",
      "252/1094 ━━━━━━━━━━━━━━━━━━━━ 2312.8s - loss: 0.0024\n",
      "253/1094 ━━━━━━━━━━━━━━━━━━━━ 2321.5s - loss: 0.0024\n",
      "254/1094 ━━━━━━━━━━━━━━━━━━━━ 2331.3s - loss: 0.0024\n",
      "255/1094 ━━━━━━━━━━━━━━━━━━━━ 2340.9s - loss: 0.0024\n",
      "256/1094 ━━━━━━━━━━━━━━━━━━━━ 2350.6s - loss: 0.0024\n",
      "257/1094 ━━━━━━━━━━━━━━━━━━━━ 2361.0s - loss: 0.0024\n",
      "258/1094 ━━━━━━━━━━━━━━━━━━━━ 2371.4s - loss: 0.0024\n",
      "259/1094 ━━━━━━━━━━━━━━━━━━━━ 2381.7s - loss: 0.0023\n",
      "260/1094 ━━━━━━━━━━━━━━━━━━━━ 2391.9s - loss: 0.0023\n",
      "261/1094 ━━━━━━━━━━━━━━━━━━━━ 2401.2s - loss: 0.0023\n",
      "262/1094 ━━━━━━━━━━━━━━━━━━━━ 2410.3s - loss: 0.0024\n",
      "263/1094 ━━━━━━━━━━━━━━━━━━━━ 2420.5s - loss: 0.0023\n",
      "264/1094 ━━━━━━━━━━━━━━━━━━━━ 2429.2s - loss: 0.0023\n",
      "265/1094 ━━━━━━━━━━━━━━━━━━━━ 2438.0s - loss: 0.0023\n",
      "266/1094 ━━━━━━━━━━━━━━━━━━━━ 2447.7s - loss: 0.0023\n",
      "267/1094 ━━━━━━━━━━━━━━━━━━━━ 2456.6s - loss: 0.0022\n",
      "268/1094 ━━━━━━━━━━━━━━━━━━━━ 2465.2s - loss: 0.0027\n",
      "269/1094 ━━━━━━━━━━━━━━━━━━━━ 2473.2s - loss: 0.0022\n",
      "270/1094 ━━━━━━━━━━━━━━━━━━━━ 2481.8s - loss: 0.0022\n",
      "271/1094 ━━━━━━━━━━━━━━━━━━━━ 2490.6s - loss: 0.0022\n",
      "272/1094 ━━━━━━━━━━━━━━━━━━━━ 2500.0s - loss: 0.0022\n",
      "273/1094 ━━━━━━━━━━━━━━━━━━━━ 2508.6s - loss: 0.0022\n",
      "274/1094 ━━━━━━━━━━━━━━━━━━━━ 2518.5s - loss: 0.0022\n",
      "275/1094 ━━━━━━━━━━━━━━━━━━━━ 2529.5s - loss: 0.0022\n",
      "276/1094 ━━━━━━━━━━━━━━━━━━━━ 2538.2s - loss: 0.0022\n",
      "277/1094 ━━━━━━━━━━━━━━━━━━━━ 2548.4s - loss: 0.0022\n",
      "278/1094 ━━━━━━━━━━━━━━━━━━━━ 2556.6s - loss: 0.0021\n",
      "279/1094 ━━━━━━━━━━━━━━━━━━━━ 2566.8s - loss: 0.0021\n",
      "280/1094 ━━━━━━━━━━━━━━━━━━━━ 2575.7s - loss: 0.0021\n",
      "281/1094 ━━━━━━━━━━━━━━━━━━━━ 2583.9s - loss: 0.0022\n",
      "282/1094 ━━━━━━━━━━━━━━━━━━━━ 2592.8s - loss: 0.0021\n",
      "283/1094 ━━━━━━━━━━━━━━━━━━━━ 2601.8s - loss: 0.0021\n",
      "284/1094 ━━━━━━━━━━━━━━━━━━━━ 2611.7s - loss: 0.0021\n",
      "285/1094 ━━━━━━━━━━━━━━━━━━━━ 2620.5s - loss: 0.0021\n",
      "286/1094 ━━━━━━━━━━━━━━━━━━━━ 2629.0s - loss: 0.0021\n",
      "287/1094 ━━━━━━━━━━━━━━━━━━━━ 2639.2s - loss: 0.0020\n",
      "288/1094 ━━━━━━━━━━━━━━━━━━━━ 2648.4s - loss: 0.0021\n",
      "289/1094 ━━━━━━━━━━━━━━━━━━━━ 2659.1s - loss: 0.0021\n",
      "290/1094 ━━━━━━━━━━━━━━━━━━━━ 2668.6s - loss: 0.0020\n",
      "291/1094 ━━━━━━━━━━━━━━━━━━━━ 2678.2s - loss: 0.0020\n",
      "292/1094 ━━━━━━━━━━━━━━━━━━━━ 2688.1s - loss: 0.0020\n",
      "293/1094 ━━━━━━━━━━━━━━━━━━━━ 2697.5s - loss: 0.0020\n",
      "294/1094 ━━━━━━━━━━━━━━━━━━━━ 2707.3s - loss: 0.0020\n",
      "295/1094 ━━━━━━━━━━━━━━━━━━━━ 2716.3s - loss: 0.0019\n",
      "296/1094 ━━━━━━━━━━━━━━━━━━━━ 2726.4s - loss: 0.0020\n",
      "297/1094 ━━━━━━━━━━━━━━━━━━━━ 2733.8s - loss: 0.0020\n",
      "298/1094 ━━━━━━━━━━━━━━━━━━━━ 2742.1s - loss: 0.0019\n",
      "299/1094 ━━━━━━━━━━━━━━━━━━━━ 2750.7s - loss: 0.0019\n",
      "300/1094 ━━━━━━━━━━━━━━━━━━━━ 2759.3s - loss: 0.0019\n",
      "301/1094 ━━━━━━━━━━━━━━━━━━━━ 2769.0s - loss: 0.0019\n",
      "302/1094 ━━━━━━━━━━━━━━━━━━━━ 2776.5s - loss: 0.0020\n",
      "303/1094 ━━━━━━━━━━━━━━━━━━━━ 2785.8s - loss: 0.0019\n",
      "304/1094 ━━━━━━━━━━━━━━━━━━━━ 2794.1s - loss: 0.0019\n",
      "305/1094 ━━━━━━━━━━━━━━━━━━━━ 2803.9s - loss: 0.0019\n",
      "306/1094 ━━━━━━━━━━━━━━━━━━━━ 2815.7s - loss: 0.0019\n",
      "307/1094 ━━━━━━━━━━━━━━━━━━━━ 2826.9s - loss: 0.0019\n",
      "308/1094 ━━━━━━━━━━━━━━━━━━━━ 2836.6s - loss: 0.0018\n",
      "309/1094 ━━━━━━━━━━━━━━━━━━━━ 2846.7s - loss: 0.0018\n",
      "310/1094 ━━━━━━━━━━━━━━━━━━━━ 2856.3s - loss: 0.0019\n",
      "311/1094 ━━━━━━━━━━━━━━━━━━━━ 2866.4s - loss: 0.0018\n",
      "312/1094 ━━━━━━━━━━━━━━━━━━━━ 2875.7s - loss: 0.0018\n",
      "313/1094 ━━━━━━━━━━━━━━━━━━━━ 2885.3s - loss: 0.0019\n",
      "314/1094 ━━━━━━━━━━━━━━━━━━━━ 2893.1s - loss: 0.0018\n",
      "315/1094 ━━━━━━━━━━━━━━━━━━━━ 2901.8s - loss: 0.0018\n",
      "316/1094 ━━━━━━━━━━━━━━━━━━━━ 2910.5s - loss: 0.0018\n",
      "317/1094 ━━━━━━━━━━━━━━━━━━━━ 2919.2s - loss: 0.0018\n",
      "318/1094 ━━━━━━━━━━━━━━━━━━━━ 2927.4s - loss: 0.0018\n",
      "319/1094 ━━━━━━━━━━━━━━━━━━━━ 2936.4s - loss: 0.0018\n",
      "320/1094 ━━━━━━━━━━━━━━━━━━━━ 2945.2s - loss: 0.0017\n",
      "321/1094 ━━━━━━━━━━━━━━━━━━━━ 2953.9s - loss: 0.0018\n",
      "322/1094 ━━━━━━━━━━━━━━━━━━━━ 2964.3s - loss: 0.0018\n",
      "323/1094 ━━━━━━━━━━━━━━━━━━━━ 2972.9s - loss: 0.0018\n",
      "324/1094 ━━━━━━━━━━━━━━━━━━━━ 2984.1s - loss: 0.0017\n",
      "325/1094 ━━━━━━━━━━━━━━━━━━━━ 2994.3s - loss: 0.0017\n",
      "326/1094 ━━━━━━━━━━━━━━━━━━━━ 3004.3s - loss: 0.0017\n",
      "327/1094 ━━━━━━━━━━━━━━━━━━━━ 3012.7s - loss: 0.0017\n",
      "328/1094 ━━━━━━━━━━━━━━━━━━━━ 3022.2s - loss: 0.0017\n",
      "329/1094 ━━━━━━━━━━━━━━━━━━━━ 3032.0s - loss: 0.0017\n",
      "330/1094 ━━━━━━━━━━━━━━━━━━━━ 3042.3s - loss: 0.0017\n",
      "331/1094 ━━━━━━━━━━━━━━━━━━━━ 3050.8s - loss: 0.0017\n",
      "332/1094 ━━━━━━━━━━━━━━━━━━━━ 3059.3s - loss: 0.0017\n",
      "333/1094 ━━━━━━━━━━━━━━━━━━━━ 3067.9s - loss: 0.0017\n",
      "334/1094 ━━━━━━━━━━━━━━━━━━━━ 3078.5s - loss: 0.0017\n",
      "335/1094 ━━━━━━━━━━━━━━━━━━━━ 3086.7s - loss: 0.0017\n",
      "336/1094 ━━━━━━━━━━━━━━━━━━━━ 3095.1s - loss: 0.0017\n",
      "337/1094 ━━━━━━━━━━━━━━━━━━━━ 3104.7s - loss: 0.0017\n",
      "338/1094 ━━━━━━━━━━━━━━━━━━━━ 3115.6s - loss: 0.0016\n",
      "339/1094 ━━━━━━━━━━━━━━━━━━━━ 3124.3s - loss: 0.0016\n",
      "340/1094 ━━━━━━━━━━━━━━━━━━━━ 3133.5s - loss: 0.0017\n",
      "341/1094 ━━━━━━━━━━━━━━━━━━━━ 3142.5s - loss: 0.0016\n",
      "342/1094 ━━━━━━━━━━━━━━━━━━━━ 3153.2s - loss: 0.0016\n",
      "343/1094 ━━━━━━━━━━━━━━━━━━━━ 3165.2s - loss: 0.0016\n",
      "344/1094 ━━━━━━━━━━━━━━━━━━━━ 3175.1s - loss: 0.0016\n",
      "345/1094 ━━━━━━━━━━━━━━━━━━━━ 3186.4s - loss: 0.0016\n",
      "346/1094 ━━━━━━━━━━━━━━━━━━━━ 3195.7s - loss: 0.0016\n",
      "347/1094 ━━━━━━━━━━━━━━━━━━━━ 3205.2s - loss: 0.0016\n",
      "348/1094 ━━━━━━━━━━━━━━━━━━━━ 3213.4s - loss: 0.0016\n",
      "349/1094 ━━━━━━━━━━━━━━━━━━━━ 3221.7s - loss: 0.0016\n",
      "350/1094 ━━━━━━━━━━━━━━━━━━━━ 3230.3s - loss: 0.0015\n",
      "351/1094 ━━━━━━━━━━━━━━━━━━━━ 3240.4s - loss: 0.0015\n",
      "352/1094 ━━━━━━━━━━━━━━━━━━━━ 3250.9s - loss: 0.0015\n",
      "353/1094 ━━━━━━━━━━━━━━━━━━━━ 3259.5s - loss: 0.0015\n",
      "354/1094 ━━━━━━━━━━━━━━━━━━━━ 3268.6s - loss: 0.0016\n",
      "355/1094 ━━━━━━━━━━━━━━━━━━━━ 3279.6s - loss: 0.0015\n",
      "356/1094 ━━━━━━━━━━━━━━━━━━━━ 3289.6s - loss: 0.0015\n",
      "357/1094 ━━━━━━━━━━━━━━━━━━━━ 3299.2s - loss: 0.0015\n",
      "358/1094 ━━━━━━━━━━━━━━━━━━━━ 3309.8s - loss: 0.0015\n",
      "359/1094 ━━━━━━━━━━━━━━━━━━━━ 3321.0s - loss: 0.0015\n",
      "360/1094 ━━━━━━━━━━━━━━━━━━━━ 3331.7s - loss: 0.0015\n",
      "361/1094 ━━━━━━━━━━━━━━━━━━━━ 3341.8s - loss: 0.0015\n",
      "362/1094 ━━━━━━━━━━━━━━━━━━━━ 3351.9s - loss: 0.0015\n",
      "363/1094 ━━━━━━━━━━━━━━━━━━━━ 3361.6s - loss: 0.0015\n",
      "364/1094 ━━━━━━━━━━━━━━━━━━━━ 3369.9s - loss: 0.0015\n",
      "365/1094 ━━━━━━━━━━━━━━━━━━━━ 3380.7s - loss: 0.0015\n",
      "366/1094 ━━━━━━━━━━━━━━━━━━━━ 3389.5s - loss: 0.0015\n",
      "367/1094 ━━━━━━━━━━━━━━━━━━━━ 3398.9s - loss: 0.0014\n",
      "368/1094 ━━━━━━━━━━━━━━━━━━━━ 3408.9s - loss: 0.0015\n",
      "369/1094 ━━━━━━━━━━━━━━━━━━━━ 3418.9s - loss: 0.0015\n",
      "370/1094 ━━━━━━━━━━━━━━━━━━━━ 3430.7s - loss: 0.0014\n",
      "371/1094 ━━━━━━━━━━━━━━━━━━━━ 3440.8s - loss: 0.0014\n",
      "372/1094 ━━━━━━━━━━━━━━━━━━━━ 3450.2s - loss: 0.0014\n",
      "373/1094 ━━━━━━━━━━━━━━━━━━━━ 3460.1s - loss: 0.0014\n",
      "374/1094 ━━━━━━━━━━━━━━━━━━━━ 3469.6s - loss: 0.0014\n",
      "375/1094 ━━━━━━━━━━━━━━━━━━━━ 3480.1s - loss: 0.0014\n",
      "376/1094 ━━━━━━━━━━━━━━━━━━━━ 3489.0s - loss: 0.0014\n",
      "377/1094 ━━━━━━━━━━━━━━━━━━━━ 3498.5s - loss: 0.0014\n",
      "378/1094 ━━━━━━━━━━━━━━━━━━━━ 3508.0s - loss: 0.0014\n",
      "379/1094 ━━━━━━━━━━━━━━━━━━━━ 3516.4s - loss: 0.0014\n",
      "380/1094 ━━━━━━━━━━━━━━━━━━━━ 3526.4s - loss: 0.0014\n",
      "381/1094 ━━━━━━━━━━━━━━━━━━━━ 3537.8s - loss: 0.0014\n",
      "382/1094 ━━━━━━━━━━━━━━━━━━━━ 3549.0s - loss: 0.0014\n",
      "383/1094 ━━━━━━━━━━━━━━━━━━━━ 3558.6s - loss: 0.0014\n",
      "384/1094 ━━━━━━━━━━━━━━━━━━━━ 3569.9s - loss: 0.0014\n",
      "385/1094 ━━━━━━━━━━━━━━━━━━━━ 3580.9s - loss: 0.0014\n",
      "386/1094 ━━━━━━━━━━━━━━━━━━━━ 3589.7s - loss: 0.0013\n",
      "387/1094 ━━━━━━━━━━━━━━━━━━━━ 3598.5s - loss: 0.0013\n",
      "388/1094 ━━━━━━━━━━━━━━━━━━━━ 3607.2s - loss: 0.0014\n",
      "389/1094 ━━━━━━━━━━━━━━━━━━━━ 3616.1s - loss: 0.0013\n",
      "390/1094 ━━━━━━━━━━━━━━━━━━━━ 3625.2s - loss: 0.0014\n",
      "391/1094 ━━━━━━━━━━━━━━━━━━━━ 3634.1s - loss: 0.0013\n",
      "392/1094 ━━━━━━━━━━━━━━━━━━━━ 3643.2s - loss: 0.0014\n",
      "393/1094 ━━━━━━━━━━━━━━━━━━━━ 3652.4s - loss: 0.0013\n",
      "394/1094 ━━━━━━━━━━━━━━━━━━━━ 3660.6s - loss: 0.0013\n",
      "395/1094 ━━━━━━━━━━━━━━━━━━━━ 3671.2s - loss: 0.0013\n",
      "396/1094 ━━━━━━━━━━━━━━━━━━━━ 3680.1s - loss: 0.0013\n",
      "397/1094 ━━━━━━━━━━━━━━━━━━━━ 3690.6s - loss: 0.0013\n",
      "398/1094 ━━━━━━━━━━━━━━━━━━━━ 3702.3s - loss: 0.0013\n",
      "399/1094 ━━━━━━━━━━━━━━━━━━━━ 3713.3s - loss: 0.0013\n",
      "400/1094 ━━━━━━━━━━━━━━━━━━━━ 3723.7s - loss: 0.0013\n",
      "401/1094 ━━━━━━━━━━━━━━━━━━━━ 3733.2s - loss: 0.0013\n",
      "402/1094 ━━━━━━━━━━━━━━━━━━━━ 3744.0s - loss: 0.0013\n",
      "403/1094 ━━━━━━━━━━━━━━━━━━━━ 3753.5s - loss: 0.0013\n",
      "404/1094 ━━━━━━━━━━━━━━━━━━━━ 3762.0s - loss: 0.0013\n",
      "405/1094 ━━━━━━━━━━━━━━━━━━━━ 3771.6s - loss: 0.0013\n",
      "406/1094 ━━━━━━━━━━━━━━━━━━━━ 3780.5s - loss: 0.0013\n",
      "407/1094 ━━━━━━━━━━━━━━━━━━━━ 3788.8s - loss: 0.0013\n",
      "408/1094 ━━━━━━━━━━━━━━━━━━━━ 3798.4s - loss: 0.0012\n",
      "409/1094 ━━━━━━━━━━━━━━━━━━━━ 3806.8s - loss: 0.0013\n",
      "410/1094 ━━━━━━━━━━━━━━━━━━━━ 3816.1s - loss: 0.0013\n",
      "411/1094 ━━━━━━━━━━━━━━━━━━━━ 3824.8s - loss: 0.0012\n",
      "412/1094 ━━━━━━━━━━━━━━━━━━━━ 3834.7s - loss: 0.0013\n",
      "413/1094 ━━━━━━━━━━━━━━━━━━━━ 3846.3s - loss: 0.0012\n",
      "414/1094 ━━━━━━━━━━━━━━━━━━━━ 3856.4s - loss: 0.0012\n",
      "415/1094 ━━━━━━━━━━━━━━━━━━━━ 3867.9s - loss: 0.0012\n",
      "416/1094 ━━━━━━━━━━━━━━━━━━━━ 3877.0s - loss: 0.0013\n",
      "417/1094 ━━━━━━━━━━━━━━━━━━━━ 3886.8s - loss: 0.0012\n",
      "418/1094 ━━━━━━━━━━━━━━━━━━━━ 3897.6s - loss: 0.0012\n",
      "419/1094 ━━━━━━━━━━━━━━━━━━━━ 3906.6s - loss: 0.0012\n",
      "420/1094 ━━━━━━━━━━━━━━━━━━━━ 3915.6s - loss: 0.0012\n",
      "421/1094 ━━━━━━━━━━━━━━━━━━━━ 3924.7s - loss: 0.0012\n",
      "422/1094 ━━━━━━━━━━━━━━━━━━━━ 3933.7s - loss: 0.0012\n",
      "423/1094 ━━━━━━━━━━━━━━━━━━━━ 3945.0s - loss: 0.0012\n",
      "424/1094 ━━━━━━━━━━━━━━━━━━━━ 3956.0s - loss: 0.0012\n",
      "425/1094 ━━━━━━━━━━━━━━━━━━━━ 3965.7s - loss: 0.0012\n",
      "426/1094 ━━━━━━━━━━━━━━━━━━━━ 3975.6s - loss: 0.0012\n",
      "427/1094 ━━━━━━━━━━━━━━━━━━━━ 3985.5s - loss: 0.0012\n",
      "428/1094 ━━━━━━━━━━━━━━━━━━━━ 3996.2s - loss: 0.0012\n",
      "429/1094 ━━━━━━━━━━━━━━━━━━━━ 4004.8s - loss: 0.0012\n",
      "430/1094 ━━━━━━━━━━━━━━━━━━━━ 4013.9s - loss: 0.0012\n",
      "431/1094 ━━━━━━━━━━━━━━━━━━━━ 4023.0s - loss: 0.0012\n",
      "432/1094 ━━━━━━━━━━━━━━━━━━━━ 4032.7s - loss: 0.0012\n",
      "433/1094 ━━━━━━━━━━━━━━━━━━━━ 4042.7s - loss: 0.0012\n",
      "434/1094 ━━━━━━━━━━━━━━━━━━━━ 4051.8s - loss: 0.0012\n",
      "435/1094 ━━━━━━━━━━━━━━━━━━━━ 4062.0s - loss: 0.0012\n",
      "436/1094 ━━━━━━━━━━━━━━━━━━━━ 4073.0s - loss: 0.0012\n",
      "437/1094 ━━━━━━━━━━━━━━━━━━━━ 4083.2s - loss: 0.0011\n",
      "438/1094 ━━━━━━━━━━━━━━━━━━━━ 4092.7s - loss: 0.0011\n",
      "439/1094 ━━━━━━━━━━━━━━━━━━━━ 4101.8s - loss: 0.0011\n",
      "440/1094 ━━━━━━━━━━━━━━━━━━━━ 4111.3s - loss: 0.0011\n",
      "441/1094 ━━━━━━━━━━━━━━━━━━━━ 4120.8s - loss: 0.0011\n",
      "442/1094 ━━━━━━━━━━━━━━━━━━━━ 4129.4s - loss: 0.0012\n",
      "443/1094 ━━━━━━━━━━━━━━━━━━━━ 4138.5s - loss: 0.0011\n",
      "444/1094 ━━━━━━━━━━━━━━━━━━━━ 4147.7s - loss: 0.0011\n",
      "445/1094 ━━━━━━━━━━━━━━━━━━━━ 4156.7s - loss: 0.0011\n",
      "446/1094 ━━━━━━━━━━━━━━━━━━━━ 4166.5s - loss: 0.0011\n",
      "447/1094 ━━━━━━━━━━━━━━━━━━━━ 4176.8s - loss: 0.0011\n",
      "448/1094 ━━━━━━━━━━━━━━━━━━━━ 4188.2s - loss: 0.0011\n",
      "449/1094 ━━━━━━━━━━━━━━━━━━━━ 4199.6s - loss: 0.0011\n",
      "450/1094 ━━━━━━━━━━━━━━━━━━━━ 4208.8s - loss: 0.0011\n",
      "451/1094 ━━━━━━━━━━━━━━━━━━━━ 4217.2s - loss: 0.0011\n",
      "452/1094 ━━━━━━━━━━━━━━━━━━━━ 4226.8s - loss: 0.0011\n",
      "453/1094 ━━━━━━━━━━━━━━━━━━━━ 4237.0s - loss: 0.0011\n",
      "454/1094 ━━━━━━━━━━━━━━━━━━━━ 4247.8s - loss: 0.0011\n",
      "455/1094 ━━━━━━━━━━━━━━━━━━━━ 4257.5s - loss: 0.0011\n",
      "456/1094 ━━━━━━━━━━━━━━━━━━━━ 4267.1s - loss: 0.0011\n",
      "457/1094 ━━━━━━━━━━━━━━━━━━━━ 4276.3s - loss: 0.0011\n",
      "458/1094 ━━━━━━━━━━━━━━━━━━━━ 4285.0s - loss: 0.0011\n",
      "459/1094 ━━━━━━━━━━━━━━━━━━━━ 4294.6s - loss: 0.0011\n",
      "460/1094 ━━━━━━━━━━━━━━━━━━━━ 4307.0s - loss: 0.0011\n",
      "461/1094 ━━━━━━━━━━━━━━━━━━━━ 4316.8s - loss: 0.0011\n",
      "462/1094 ━━━━━━━━━━━━━━━━━━━━ 4326.7s - loss: 0.0011\n",
      "463/1094 ━━━━━━━━━━━━━━━━━━━━ 4336.3s - loss: 0.0011\n",
      "464/1094 ━━━━━━━━━━━━━━━━━━━━ 4345.4s - loss: 0.0011\n",
      "465/1094 ━━━━━━━━━━━━━━━━━━━━ 4355.2s - loss: 0.0011\n",
      "466/1094 ━━━━━━━━━━━━━━━━━━━━ 4364.7s - loss: 0.0010\n",
      "467/1094 ━━━━━━━━━━━━━━━━━━━━ 4373.1s - loss: 0.0011\n",
      "468/1094 ━━━━━━━━━━━━━━━━━━━━ 4381.6s - loss: 0.0011\n",
      "469/1094 ━━━━━━━━━━━━━━━━━━━━ 4389.9s - loss: 0.0010\n",
      "470/1094 ━━━━━━━━━━━━━━━━━━━━ 4400.6s - loss: 0.0010\n",
      "471/1094 ━━━━━━━━━━━━━━━━━━━━ 4411.7s - loss: 0.0010\n",
      "472/1094 ━━━━━━━━━━━━━━━━━━━━ 4420.2s - loss: 0.0010\n",
      "473/1094 ━━━━━━━━━━━━━━━━━━━━ 4430.5s - loss: 0.0010\n",
      "474/1094 ━━━━━━━━━━━━━━━━━━━━ 4441.7s - loss: 0.0010\n",
      "475/1094 ━━━━━━━━━━━━━━━━━━━━ 4452.8s - loss: 0.0010\n",
      "476/1094 ━━━━━━━━━━━━━━━━━━━━ 4462.6s - loss: 0.0010\n",
      "477/1094 ━━━━━━━━━━━━━━━━━━━━ 4473.1s - loss: 0.0010\n",
      "478/1094 ━━━━━━━━━━━━━━━━━━━━ 4485.5s - loss: 0.0010\n",
      "479/1094 ━━━━━━━━━━━━━━━━━━━━ 4495.9s - loss: 0.0010\n",
      "480/1094 ━━━━━━━━━━━━━━━━━━━━ 4506.0s - loss: 0.0010\n",
      "481/1094 ━━━━━━━━━━━━━━━━━━━━ 4516.1s - loss: 0.0010\n",
      "482/1094 ━━━━━━━━━━━━━━━━━━━━ 4526.7s - loss: 0.0010\n",
      "483/1094 ━━━━━━━━━━━━━━━━━━━━ 4536.6s - loss: 0.0010\n",
      "484/1094 ━━━━━━━━━━━━━━━━━━━━ 4546.9s - loss: 0.0010\n",
      "485/1094 ━━━━━━━━━━━━━━━━━━━━ 4556.0s - loss: 0.0010\n",
      "486/1094 ━━━━━━━━━━━━━━━━━━━━ 4564.9s - loss: 0.0010\n",
      "487/1094 ━━━━━━━━━━━━━━━━━━━━ 4574.1s - loss: 0.0010\n",
      "488/1094 ━━━━━━━━━━━━━━━━━━━━ 4583.7s - loss: 0.0010\n",
      "489/1094 ━━━━━━━━━━━━━━━━━━━━ 4595.3s - loss: 0.0010\n",
      "490/1094 ━━━━━━━━━━━━━━━━━━━━ 4603.6s - loss: 0.0010\n",
      "491/1094 ━━━━━━━━━━━━━━━━━━━━ 4613.3s - loss: 0.0010\n",
      "492/1094 ━━━━━━━━━━━━━━━━━━━━ 4628.2s - loss: 0.0010\n",
      "493/1094 ━━━━━━━━━━━━━━━━━━━━ 4639.2s - loss: 0.0010\n",
      "494/1094 ━━━━━━━━━━━━━━━━━━━━ 4650.7s - loss: 0.0010\n",
      "495/1094 ━━━━━━━━━━━━━━━━━━━━ 4661.5s - loss: 0.0009\n",
      "496/1094 ━━━━━━━━━━━━━━━━━━━━ 4673.1s - loss: 0.0010\n",
      "497/1094 ━━━━━━━━━━━━━━━━━━━━ 4683.4s - loss: 0.0010\n",
      "498/1094 ━━━━━━━━━━━━━━━━━━━━ 4692.6s - loss: 0.0009\n",
      "499/1094 ━━━━━━━━━━━━━━━━━━━━ 4701.4s - loss: 0.0010\n",
      "500/1094 ━━━━━━━━━━━━━━━━━━━━ 4710.7s - loss: 0.0009\n",
      "501/1094 ━━━━━━━━━━━━━━━━━━━━ 4720.7s - loss: 0.0010\n",
      "502/1094 ━━━━━━━━━━━━━━━━━━━━ 4730.7s - loss: 0.0010\n",
      "503/1094 ━━━━━━━━━━━━━━━━━━━━ 4741.1s - loss: 0.0010\n",
      "504/1094 ━━━━━━━━━━━━━━━━━━━━ 4751.3s - loss: 0.0010\n",
      "505/1094 ━━━━━━━━━━━━━━━━━━━━ 4762.4s - loss: 0.0009\n",
      "506/1094 ━━━━━━━━━━━━━━━━━━━━ 4773.1s - loss: 0.0009\n",
      "507/1094 ━━━━━━━━━━━━━━━━━━━━ 4783.9s - loss: 0.0009\n",
      "508/1094 ━━━━━━━━━━━━━━━━━━━━ 4792.5s - loss: 0.0009\n",
      "509/1094 ━━━━━━━━━━━━━━━━━━━━ 4802.8s - loss: 0.0009\n",
      "510/1094 ━━━━━━━━━━━━━━━━━━━━ 4811.8s - loss: 0.0009\n",
      "511/1094 ━━━━━━━━━━━━━━━━━━━━ 4822.7s - loss: 0.0009\n",
      "512/1094 ━━━━━━━━━━━━━━━━━━━━ 4832.8s - loss: 0.0009\n",
      "513/1094 ━━━━━━━━━━━━━━━━━━━━ 4842.1s - loss: 0.0009\n",
      "514/1094 ━━━━━━━━━━━━━━━━━━━━ 4852.4s - loss: 0.0009\n",
      "515/1094 ━━━━━━━━━━━━━━━━━━━━ 4864.6s - loss: 0.0009\n",
      "516/1094 ━━━━━━━━━━━━━━━━━━━━ 4880.1s - loss: 0.0009\n",
      "517/1094 ━━━━━━━━━━━━━━━━━━━━ 4890.9s - loss: 0.0009\n",
      "518/1094 ━━━━━━━━━━━━━━━━━━━━ 4901.4s - loss: 0.0009\n",
      "519/1094 ━━━━━━━━━━━━━━━━━━━━ 4909.4s - loss: 0.0009\n",
      "520/1094 ━━━━━━━━━━━━━━━━━━━━ 4917.8s - loss: 0.0009\n",
      "521/1094 ━━━━━━━━━━━━━━━━━━━━ 4926.9s - loss: 0.0009\n",
      "522/1094 ━━━━━━━━━━━━━━━━━━━━ 4935.2s - loss: 0.0009\n",
      "523/1094 ━━━━━━━━━━━━━━━━━━━━ 4943.1s - loss: 0.0009\n",
      "524/1094 ━━━━━━━━━━━━━━━━━━━━ 4953.0s - loss: 0.0009\n",
      "525/1094 ━━━━━━━━━━━━━━━━━━━━ 4962.6s - loss: 0.0009\n",
      "526/1094 ━━━━━━━━━━━━━━━━━━━━ 4971.3s - loss: 0.0009\n",
      "527/1094 ━━━━━━━━━━━━━━━━━━━━ 4980.9s - loss: 0.0009\n",
      "528/1094 ━━━━━━━━━━━━━━━━━━━━ 4991.0s - loss: 0.0009\n",
      "529/1094 ━━━━━━━━━━━━━━━━━━━━ 4999.9s - loss: 0.0009\n",
      "530/1094 ━━━━━━━━━━━━━━━━━━━━ 5009.8s - loss: 0.0009\n",
      "531/1094 ━━━━━━━━━━━━━━━━━━━━ 5017.9s - loss: 0.0009\n",
      "532/1094 ━━━━━━━━━━━━━━━━━━━━ 5028.2s - loss: 0.0009\n",
      "533/1094 ━━━━━━━━━━━━━━━━━━━━ 5036.7s - loss: 0.0009\n",
      "534/1094 ━━━━━━━━━━━━━━━━━━━━ 5045.3s - loss: 0.0009\n",
      "535/1094 ━━━━━━━━━━━━━━━━━━━━ 5054.9s - loss: 0.0009\n",
      "536/1094 ━━━━━━━━━━━━━━━━━━━━ 5063.9s - loss: 0.0009\n",
      "537/1094 ━━━━━━━━━━━━━━━━━━━━ 5073.3s - loss: 0.0009\n",
      "538/1094 ━━━━━━━━━━━━━━━━━━━━ 5081.6s - loss: 0.0009\n",
      "539/1094 ━━━━━━━━━━━━━━━━━━━━ 5089.9s - loss: 0.0009\n",
      "540/1094 ━━━━━━━━━━━━━━━━━━━━ 5097.9s - loss: 0.0009\n",
      "541/1094 ━━━━━━━━━━━━━━━━━━━━ 5106.4s - loss: 0.0009\n",
      "542/1094 ━━━━━━━━━━━━━━━━━━━━ 5116.3s - loss: 0.0009\n",
      "543/1094 ━━━━━━━━━━━━━━━━━━━━ 5124.6s - loss: 0.0008\n",
      "544/1094 ━━━━━━━━━━━━━━━━━━━━ 5132.9s - loss: 0.0009\n",
      "545/1094 ━━━━━━━━━━━━━━━━━━━━ 5141.7s - loss: 0.0008\n",
      "546/1094 ━━━━━━━━━━━━━━━━━━━━ 5152.3s - loss: 0.0008\n",
      "547/1094 ━━━━━━━━━━━━━━━━━━━━ 5162.6s - loss: 0.0008\n",
      "548/1094 ━━━━━━━━━━━━━━━━━━━━ 5172.6s - loss: 0.0008\n",
      "549/1094 ━━━━━━━━━━━━━━━━━━━━ 5183.2s - loss: 0.0008\n",
      "550/1094 ━━━━━━━━━━━━━━━━━━━━ 5193.9s - loss: 0.0008\n",
      "551/1094 ━━━━━━━━━━━━━━━━━━━━ 5204.3s - loss: 0.0008\n",
      "552/1094 ━━━━━━━━━━━━━━━━━━━━ 5215.1s - loss: 0.0008\n",
      "553/1094 ━━━━━━━━━━━━━━━━━━━━ 5225.0s - loss: 0.0008\n",
      "554/1094 ━━━━━━━━━━━━━━━━━━━━ 5234.8s - loss: 0.0008\n",
      "555/1094 ━━━━━━━━━━━━━━━━━━━━ 5242.9s - loss: 0.0008\n",
      "556/1094 ━━━━━━━━━━━━━━━━━━━━ 5253.5s - loss: 0.0008\n",
      "557/1094 ━━━━━━━━━━━━━━━━━━━━ 5261.9s - loss: 0.0008\n",
      "558/1094 ━━━━━━━━━━━━━━━━━━━━ 5270.5s - loss: 0.0008\n",
      "559/1094 ━━━━━━━━━━━━━━━━━━━━ 5280.0s - loss: 0.0008\n",
      "560/1094 ━━━━━━━━━━━━━━━━━━━━ 5289.6s - loss: 0.0008\n",
      "561/1094 ━━━━━━━━━━━━━━━━━━━━ 5300.7s - loss: 0.0008\n",
      "562/1094 ━━━━━━━━━━━━━━━━━━━━ 5309.4s - loss: 0.0008\n",
      "563/1094 ━━━━━━━━━━━━━━━━━━━━ 5318.9s - loss: 0.0008\n",
      "564/1094 ━━━━━━━━━━━━━━━━━━━━ 5328.8s - loss: 0.0008\n",
      "565/1094 ━━━━━━━━━━━━━━━━━━━━ 5338.6s - loss: 0.0008\n",
      "566/1094 ━━━━━━━━━━━━━━━━━━━━ 5348.8s - loss: 0.0008\n",
      "567/1094 ━━━━━━━━━━━━━━━━━━━━ 5360.1s - loss: 0.0008\n",
      "568/1094 ━━━━━━━━━━━━━━━━━━━━ 5370.3s - loss: 0.0008\n",
      "569/1094 ━━━━━━━━━━━━━━━━━━━━ 5379.3s - loss: 0.0008\n",
      "570/1094 ━━━━━━━━━━━━━━━━━━━━ 5390.3s - loss: 0.0008\n",
      "571/1094 ━━━━━━━━━━━━━━━━━━━━ 5400.6s - loss: 0.0008\n",
      "572/1094 ━━━━━━━━━━━━━━━━━━━━ 5410.8s - loss: 0.0008\n",
      "573/1094 ━━━━━━━━━━━━━━━━━━━━ 5421.3s - loss: 0.0008\n",
      "574/1094 ━━━━━━━━━━━━━━━━━━━━ 5431.8s - loss: 0.0008\n",
      "575/1094 ━━━━━━━━━━━━━━━━━━━━ 5442.8s - loss: 0.0008\n",
      "576/1094 ━━━━━━━━━━━━━━━━━━━━ 5451.8s - loss: 0.0008\n",
      "577/1094 ━━━━━━━━━━━━━━━━━━━━ 5460.1s - loss: 0.0008\n",
      "578/1094 ━━━━━━━━━━━━━━━━━━━━ 5468.8s - loss: 0.0008\n",
      "579/1094 ━━━━━━━━━━━━━━━━━━━━ 5478.2s - loss: 0.0008\n",
      "580/1094 ━━━━━━━━━━━━━━━━━━━━ 5487.9s - loss: 0.0008\n",
      "581/1094 ━━━━━━━━━━━━━━━━━━━━ 5497.0s - loss: 0.0008\n",
      "582/1094 ━━━━━━━━━━━━━━━━━━━━ 5505.6s - loss: 0.0008\n",
      "583/1094 ━━━━━━━━━━━━━━━━━━━━ 5514.7s - loss: 0.0008\n",
      "584/1094 ━━━━━━━━━━━━━━━━━━━━ 5524.6s - loss: 0.0008\n",
      "585/1094 ━━━━━━━━━━━━━━━━━━━━ 5533.0s - loss: 0.0008\n",
      "586/1094 ━━━━━━━━━━━━━━━━━━━━ 5542.8s - loss: 0.0008\n",
      "587/1094 ━━━━━━━━━━━━━━━━━━━━ 5552.5s - loss: 0.0008\n",
      "588/1094 ━━━━━━━━━━━━━━━━━━━━ 5560.9s - loss: 0.0008\n",
      "589/1094 ━━━━━━━━━━━━━━━━━━━━ 5570.0s - loss: 0.0008\n",
      "590/1094 ━━━━━━━━━━━━━━━━━━━━ 5579.7s - loss: 0.0008\n",
      "591/1094 ━━━━━━━━━━━━━━━━━━━━ 5588.9s - loss: 0.0008\n",
      "592/1094 ━━━━━━━━━━━━━━━━━━━━ 5599.0s - loss: 0.0008\n",
      "593/1094 ━━━━━━━━━━━━━━━━━━━━ 5607.8s - loss: 0.0008\n",
      "594/1094 ━━━━━━━━━━━━━━━━━━━━ 5617.7s - loss: 0.0007\n",
      "595/1094 ━━━━━━━━━━━━━━━━━━━━ 5626.6s - loss: 0.0008\n",
      "596/1094 ━━━━━━━━━━━━━━━━━━━━ 5636.6s - loss: 0.0007\n",
      "597/1094 ━━━━━━━━━━━━━━━━━━━━ 5647.0s - loss: 0.0007\n",
      "598/1094 ━━━━━━━━━━━━━━━━━━━━ 5657.1s - loss: 0.0007\n",
      "599/1094 ━━━━━━━━━━━━━━━━━━━━ 5666.9s - loss: 0.0007\n",
      "600/1094 ━━━━━━━━━━━━━━━━━━━━ 5676.1s - loss: 0.0007\n",
      "601/1094 ━━━━━━━━━━━━━━━━━━━━ 5684.8s - loss: 0.0007\n",
      "602/1094 ━━━━━━━━━━━━━━━━━━━━ 5694.3s - loss: 0.0007\n",
      "603/1094 ━━━━━━━━━━━━━━━━━━━━ 5704.4s - loss: 0.0007\n",
      "604/1094 ━━━━━━━━━━━━━━━━━━━━ 5714.5s - loss: 0.0007\n",
      "605/1094 ━━━━━━━━━━━━━━━━━━━━ 5724.2s - loss: 0.0007\n",
      "606/1094 ━━━━━━━━━━━━━━━━━━━━ 5733.5s - loss: 0.0007\n",
      "607/1094 ━━━━━━━━━━━━━━━━━━━━ 5742.3s - loss: 0.0007\n",
      "608/1094 ━━━━━━━━━━━━━━━━━━━━ 5752.5s - loss: 0.0007\n",
      "609/1094 ━━━━━━━━━━━━━━━━━━━━ 5761.0s - loss: 0.0007\n",
      "610/1094 ━━━━━━━━━━━━━━━━━━━━ 5770.7s - loss: 0.0007\n",
      "611/1094 ━━━━━━━━━━━━━━━━━━━━ 5780.3s - loss: 0.0007\n",
      "612/1094 ━━━━━━━━━━━━━━━━━━━━ 5788.7s - loss: 0.0007\n",
      "613/1094 ━━━━━━━━━━━━━━━━━━━━ 5799.7s - loss: 0.0007\n",
      "614/1094 ━━━━━━━━━━━━━━━━━━━━ 5810.1s - loss: 0.0007\n",
      "615/1094 ━━━━━━━━━━━━━━━━━━━━ 5820.3s - loss: 0.0007\n",
      "616/1094 ━━━━━━━━━━━━━━━━━━━━ 5830.3s - loss: 0.0007\n",
      "617/1094 ━━━━━━━━━━━━━━━━━━━━ 5839.2s - loss: 0.0007\n",
      "618/1094 ━━━━━━━━━━━━━━━━━━━━ 5847.8s - loss: 0.0007\n",
      "619/1094 ━━━━━━━━━━━━━━━━━━━━ 5858.4s - loss: 0.0007\n",
      "620/1094 ━━━━━━━━━━━━━━━━━━━━ 5867.6s - loss: 0.0007\n",
      "621/1094 ━━━━━━━━━━━━━━━━━━━━ 5876.5s - loss: 0.0007\n",
      "622/1094 ━━━━━━━━━━━━━━━━━━━━ 5886.8s - loss: 0.0007\n",
      "623/1094 ━━━━━━━━━━━━━━━━━━━━ 5895.6s - loss: 0.0007\n",
      "624/1094 ━━━━━━━━━━━━━━━━━━━━ 5905.1s - loss: 0.0007\n",
      "625/1094 ━━━━━━━━━━━━━━━━━━━━ 5914.8s - loss: 0.0007\n",
      "626/1094 ━━━━━━━━━━━━━━━━━━━━ 5923.6s - loss: 0.0007\n",
      "627/1094 ━━━━━━━━━━━━━━━━━━━━ 5931.4s - loss: 0.0007\n",
      "628/1094 ━━━━━━━━━━━━━━━━━━━━ 5940.7s - loss: 0.0007\n",
      "629/1094 ━━━━━━━━━━━━━━━━━━━━ 5949.5s - loss: 0.0007\n",
      "630/1094 ━━━━━━━━━━━━━━━━━━━━ 5957.9s - loss: 0.0007\n",
      "631/1094 ━━━━━━━━━━━━━━━━━━━━ 5965.4s - loss: 0.0007\n",
      "632/1094 ━━━━━━━━━━━━━━━━━━━━ 5974.7s - loss: 0.0007\n",
      "633/1094 ━━━━━━━━━━━━━━━━━━━━ 5983.2s - loss: 0.0007\n",
      "634/1094 ━━━━━━━━━━━━━━━━━━━━ 5991.2s - loss: 0.0007\n",
      "635/1094 ━━━━━━━━━━━━━━━━━━━━ 5998.8s - loss: 0.0007\n",
      "636/1094 ━━━━━━━━━━━━━━━━━━━━ 6006.8s - loss: 0.0007\n",
      "637/1094 ━━━━━━━━━━━━━━━━━━━━ 6014.7s - loss: 0.0007\n",
      "638/1094 ━━━━━━━━━━━━━━━━━━━━ 6023.1s - loss: 0.0007\n",
      "639/1094 ━━━━━━━━━━━━━━━━━━━━ 6032.5s - loss: 0.0007\n",
      "640/1094 ━━━━━━━━━━━━━━━━━━━━ 6040.8s - loss: 0.0007\n",
      "641/1094 ━━━━━━━━━━━━━━━━━━━━ 6049.6s - loss: 0.0007\n",
      "642/1094 ━━━━━━━━━━━━━━━━━━━━ 6056.8s - loss: 0.0007\n",
      "643/1094 ━━━━━━━━━━━━━━━━━━━━ 6064.6s - loss: 0.0007\n",
      "644/1094 ━━━━━━━━━━━━━━━━━━━━ 6072.1s - loss: 0.0007\n",
      "645/1094 ━━━━━━━━━━━━━━━━━━━━ 6080.5s - loss: 0.0007\n",
      "646/1094 ━━━━━━━━━━━━━━━━━━━━ 6088.1s - loss: 0.0007\n",
      "647/1094 ━━━━━━━━━━━━━━━━━━━━ 6096.8s - loss: 0.0007\n",
      "648/1094 ━━━━━━━━━━━━━━━━━━━━ 6105.6s - loss: 0.0007\n",
      "649/1094 ━━━━━━━━━━━━━━━━━━━━ 6114.9s - loss: 0.0007\n",
      "650/1094 ━━━━━━━━━━━━━━━━━━━━ 6122.4s - loss: 0.0007\n",
      "651/1094 ━━━━━━━━━━━━━━━━━━━━ 6130.5s - loss: 0.0007\n",
      "652/1094 ━━━━━━━━━━━━━━━━━━━━ 6139.5s - loss: 0.0007\n",
      "653/1094 ━━━━━━━━━━━━━━━━━━━━ 6148.3s - loss: 0.0007\n",
      "654/1094 ━━━━━━━━━━━━━━━━━━━━ 6156.9s - loss: 0.0007\n",
      "655/1094 ━━━━━━━━━━━━━━━━━━━━ 6165.2s - loss: 0.0007\n",
      "656/1094 ━━━━━━━━━━━━━━━━━━━━ 6172.8s - loss: 0.0007\n",
      "657/1094 ━━━━━━━━━━━━━━━━━━━━ 6182.4s - loss: 0.0007\n",
      "658/1094 ━━━━━━━━━━━━━━━━━━━━ 6189.5s - loss: 0.0007\n",
      "659/1094 ━━━━━━━━━━━━━━━━━━━━ 6198.0s - loss: 0.0007\n",
      "660/1094 ━━━━━━━━━━━━━━━━━━━━ 6206.6s - loss: 0.0006\n",
      "661/1094 ━━━━━━━━━━━━━━━━━━━━ 6215.0s - loss: 0.0006\n",
      "662/1094 ━━━━━━━━━━━━━━━━━━━━ 6222.5s - loss: 0.0006\n",
      "663/1094 ━━━━━━━━━━━━━━━━━━━━ 6231.9s - loss: 0.0006\n",
      "664/1094 ━━━━━━━━━━━━━━━━━━━━ 6240.0s - loss: 0.0006\n",
      "665/1094 ━━━━━━━━━━━━━━━━━━━━ 6248.9s - loss: 0.0006\n",
      "666/1094 ━━━━━━━━━━━━━━━━━━━━ 6257.4s - loss: 0.0006\n",
      "667/1094 ━━━━━━━━━━━━━━━━━━━━ 6264.7s - loss: 0.0006\n",
      "668/1094 ━━━━━━━━━━━━━━━━━━━━ 6272.7s - loss: 0.0006\n",
      "669/1094 ━━━━━━━━━━━━━━━━━━━━ 6281.6s - loss: 0.0006\n",
      "670/1094 ━━━━━━━━━━━━━━━━━━━━ 6289.8s - loss: 0.0006\n",
      "671/1094 ━━━━━━━━━━━━━━━━━━━━ 6297.9s - loss: 0.0006\n",
      "672/1094 ━━━━━━━━━━━━━━━━━━━━ 6307.1s - loss: 0.0006\n",
      "673/1094 ━━━━━━━━━━━━━━━━━━━━ 6315.1s - loss: 0.0006\n",
      "674/1094 ━━━━━━━━━━━━━━━━━━━━ 6324.6s - loss: 0.0006\n",
      "675/1094 ━━━━━━━━━━━━━━━━━━━━ 6332.5s - loss: 0.0006\n",
      "676/1094 ━━━━━━━━━━━━━━━━━━━━ 6340.6s - loss: 0.0006\n",
      "677/1094 ━━━━━━━━━━━━━━━━━━━━ 6349.0s - loss: 0.0006\n",
      "678/1094 ━━━━━━━━━━━━━━━━━━━━ 6357.9s - loss: 0.0006\n",
      "679/1094 ━━━━━━━━━━━━━━━━━━━━ 6365.2s - loss: 0.0006\n",
      "680/1094 ━━━━━━━━━━━━━━━━━━━━ 6373.3s - loss: 0.0006\n",
      "681/1094 ━━━━━━━━━━━━━━━━━━━━ 6382.5s - loss: 0.0006\n",
      "682/1094 ━━━━━━━━━━━━━━━━━━━━ 6390.7s - loss: 0.0006\n",
      "683/1094 ━━━━━━━━━━━━━━━━━━━━ 6398.9s - loss: 0.0006\n",
      "684/1094 ━━━━━━━━━━━━━━━━━━━━ 6408.4s - loss: 0.0006\n",
      "685/1094 ━━━━━━━━━━━━━━━━━━━━ 6416.7s - loss: 0.0006\n",
      "686/1094 ━━━━━━━━━━━━━━━━━━━━ 6425.4s - loss: 0.0006\n",
      "687/1094 ━━━━━━━━━━━━━━━━━━━━ 6434.7s - loss: 0.0006\n",
      "688/1094 ━━━━━━━━━━━━━━━━━━━━ 6442.3s - loss: 0.0006\n",
      "689/1094 ━━━━━━━━━━━━━━━━━━━━ 6451.7s - loss: 0.0006\n",
      "690/1094 ━━━━━━━━━━━━━━━━━━━━ 6459.0s - loss: 0.0006\n",
      "691/1094 ━━━━━━━━━━━━━━━━━━━━ 6467.0s - loss: 0.0006\n",
      "692/1094 ━━━━━━━━━━━━━━━━━━━━ 6476.1s - loss: 0.0006\n",
      "693/1094 ━━━━━━━━━━━━━━━━━━━━ 6484.8s - loss: 0.0006\n",
      "694/1094 ━━━━━━━━━━━━━━━━━━━━ 6492.5s - loss: 0.0006\n",
      "695/1094 ━━━━━━━━━━━━━━━━━━━━ 6501.7s - loss: 0.0006\n",
      "696/1094 ━━━━━━━━━━━━━━━━━━━━ 6509.3s - loss: 0.0006\n",
      "697/1094 ━━━━━━━━━━━━━━━━━━━━ 6517.5s - loss: 0.0006\n",
      "698/1094 ━━━━━━━━━━━━━━━━━━━━ 6525.7s - loss: 0.0006\n",
      "699/1094 ━━━━━━━━━━━━━━━━━━━━ 6534.0s - loss: 0.0006\n",
      "700/1094 ━━━━━━━━━━━━━━━━━━━━ 6541.6s - loss: 0.0006\n",
      "701/1094 ━━━━━━━━━━━━━━━━━━━━ 6549.3s - loss: 0.0006\n",
      "702/1094 ━━━━━━━━━━━━━━━━━━━━ 6557.8s - loss: 0.0006\n",
      "703/1094 ━━━━━━━━━━━━━━━━━━━━ 6566.1s - loss: 0.0006\n",
      "704/1094 ━━━━━━━━━━━━━━━━━━━━ 6573.1s - loss: 0.0006\n",
      "705/1094 ━━━━━━━━━━━━━━━━━━━━ 6582.9s - loss: 0.0006\n",
      "706/1094 ━━━━━━━━━━━━━━━━━━━━ 6590.6s - loss: 0.0006\n",
      "707/1094 ━━━━━━━━━━━━━━━━━━━━ 6598.5s - loss: 0.0006\n",
      "708/1094 ━━━━━━━━━━━━━━━━━━━━ 6607.6s - loss: 0.0006\n",
      "709/1094 ━━━━━━━━━━━━━━━━━━━━ 6616.1s - loss: 0.0006\n",
      "710/1094 ━━━━━━━━━━━━━━━━━━━━ 6625.2s - loss: 0.0006\n",
      "711/1094 ━━━━━━━━━━━━━━━━━━━━ 6633.1s - loss: 0.0006\n",
      "712/1094 ━━━━━━━━━━━━━━━━━━━━ 6641.4s - loss: 0.0006\n",
      "713/1094 ━━━━━━━━━━━━━━━━━━━━ 6650.3s - loss: 0.0006\n",
      "714/1094 ━━━━━━━━━━━━━━━━━━━━ 6659.3s - loss: 0.0006\n",
      "715/1094 ━━━━━━━━━━━━━━━━━━━━ 6667.3s - loss: 0.0006\n",
      "716/1094 ━━━━━━━━━━━━━━━━━━━━ 6676.5s - loss: 0.0006\n",
      "717/1094 ━━━━━━━━━━━━━━━━━━━━ 6683.7s - loss: 0.0006\n",
      "718/1094 ━━━━━━━━━━━━━━━━━━━━ 6691.1s - loss: 0.0006\n",
      "719/1094 ━━━━━━━━━━━━━━━━━━━━ 6699.6s - loss: 0.0006\n",
      "720/1094 ━━━━━━━━━━━━━━━━━━━━ 6708.0s - loss: 0.0006\n",
      "721/1094 ━━━━━━━━━━━━━━━━━━━━ 6715.3s - loss: 0.0006\n",
      "722/1094 ━━━━━━━━━━━━━━━━━━━━ 6724.3s - loss: 0.0006\n",
      "723/1094 ━━━━━━━━━━━━━━━━━━━━ 6733.1s - loss: 0.0006\n",
      "724/1094 ━━━━━━━━━━━━━━━━━━━━ 6742.5s - loss: 0.0006\n",
      "725/1094 ━━━━━━━━━━━━━━━━━━━━ 6751.0s - loss: 0.0006\n",
      "726/1094 ━━━━━━━━━━━━━━━━━━━━ 6759.9s - loss: 0.0006\n",
      "727/1094 ━━━━━━━━━━━━━━━━━━━━ 6768.6s - loss: 0.0006\n",
      "728/1094 ━━━━━━━━━━━━━━━━━━━━ 6777.1s - loss: 0.0006\n",
      "729/1094 ━━━━━━━━━━━━━━━━━━━━ 6785.1s - loss: 0.0006\n",
      "730/1094 ━━━━━━━━━━━━━━━━━━━━ 6792.9s - loss: 0.0006\n",
      "731/1094 ━━━━━━━━━━━━━━━━━━━━ 6802.7s - loss: 0.0006\n",
      "732/1094 ━━━━━━━━━━━━━━━━━━━━ 6811.2s - loss: 0.0006\n",
      "733/1094 ━━━━━━━━━━━━━━━━━━━━ 6819.9s - loss: 0.0006\n",
      "734/1094 ━━━━━━━━━━━━━━━━━━━━ 6828.3s - loss: 0.0006\n",
      "735/1094 ━━━━━━━━━━━━━━━━━━━━ 6837.0s - loss: 0.0005\n",
      "736/1094 ━━━━━━━━━━━━━━━━━━━━ 6845.0s - loss: 0.0006\n",
      "737/1094 ━━━━━━━━━━━━━━━━━━━━ 6854.9s - loss: 0.0006\n",
      "738/1094 ━━━━━━━━━━━━━━━━━━━━ 6863.2s - loss: 0.0006\n",
      "739/1094 ━━━━━━━━━━━━━━━━━━━━ 6871.5s - loss: 0.0006\n",
      "740/1094 ━━━━━━━━━━━━━━━━━━━━ 6879.4s - loss: 0.0006\n",
      "741/1094 ━━━━━━━━━━━━━━━━━━━━ 6888.2s - loss: 0.0006\n",
      "742/1094 ━━━━━━━━━━━━━━━━━━━━ 6896.1s - loss: 0.0006\n",
      "743/1094 ━━━━━━━━━━━━━━━━━━━━ 6905.5s - loss: 0.0005\n",
      "744/1094 ━━━━━━━━━━━━━━━━━━━━ 6913.9s - loss: 0.0005\n",
      "745/1094 ━━━━━━━━━━━━━━━━━━━━ 6921.4s - loss: 0.0005\n",
      "746/1094 ━━━━━━━━━━━━━━━━━━━━ 6929.0s - loss: 0.0005\n",
      "747/1094 ━━━━━━━━━━━━━━━━━━━━ 6936.1s - loss: 0.0005\n",
      "748/1094 ━━━━━━━━━━━━━━━━━━━━ 6943.5s - loss: 0.0005\n",
      "749/1094 ━━━━━━━━━━━━━━━━━━━━ 6950.3s - loss: 0.0005\n",
      "750/1094 ━━━━━━━━━━━━━━━━━━━━ 6957.2s - loss: 0.0005\n",
      "751/1094 ━━━━━━━━━━━━━━━━━━━━ 6964.7s - loss: 0.0005\n",
      "752/1094 ━━━━━━━━━━━━━━━━━━━━ 6971.6s - loss: 0.0005\n",
      "753/1094 ━━━━━━━━━━━━━━━━━━━━ 6978.8s - loss: 0.0005\n",
      "754/1094 ━━━━━━━━━━━━━━━━━━━━ 6986.1s - loss: 0.0005\n",
      "755/1094 ━━━━━━━━━━━━━━━━━━━━ 6994.1s - loss: 0.0005\n",
      "756/1094 ━━━━━━━━━━━━━━━━━━━━ 7001.0s - loss: 0.0005\n",
      "757/1094 ━━━━━━━━━━━━━━━━━━━━ 7008.5s - loss: 0.0005\n",
      "758/1094 ━━━━━━━━━━━━━━━━━━━━ 7015.3s - loss: 0.0005\n",
      "759/1094 ━━━━━━━━━━━━━━━━━━━━ 7021.5s - loss: 0.0005\n",
      "760/1094 ━━━━━━━━━━━━━━━━━━━━ 7029.0s - loss: 0.0005\n",
      "761/1094 ━━━━━━━━━━━━━━━━━━━━ 7036.1s - loss: 0.0005\n",
      "762/1094 ━━━━━━━━━━━━━━━━━━━━ 7043.2s - loss: 0.0005\n",
      "763/1094 ━━━━━━━━━━━━━━━━━━━━ 7049.6s - loss: 0.0005\n",
      "764/1094 ━━━━━━━━━━━━━━━━━━━━ 7057.0s - loss: 0.0005\n",
      "765/1094 ━━━━━━━━━━━━━━━━━━━━ 7064.1s - loss: 0.0005\n",
      "766/1094 ━━━━━━━━━━━━━━━━━━━━ 7072.0s - loss: 0.0005\n",
      "767/1094 ━━━━━━━━━━━━━━━━━━━━ 7079.3s - loss: 0.0005\n",
      "768/1094 ━━━━━━━━━━━━━━━━━━━━ 7086.5s - loss: 0.0005\n",
      "769/1094 ━━━━━━━━━━━━━━━━━━━━ 7093.9s - loss: 0.0005\n",
      "770/1094 ━━━━━━━━━━━━━━━━━━━━ 7101.5s - loss: 0.0005\n",
      "771/1094 ━━━━━━━━━━━━━━━━━━━━ 7109.7s - loss: 0.0005\n",
      "772/1094 ━━━━━━━━━━━━━━━━━━━━ 7118.8s - loss: 0.0005\n",
      "773/1094 ━━━━━━━━━━━━━━━━━━━━ 7127.7s - loss: 0.0005\n",
      "774/1094 ━━━━━━━━━━━━━━━━━━━━ 7135.4s - loss: 0.0005\n",
      "775/1094 ━━━━━━━━━━━━━━━━━━━━ 7142.8s - loss: 0.0005\n",
      "776/1094 ━━━━━━━━━━━━━━━━━━━━ 7149.8s - loss: 0.0005\n",
      "777/1094 ━━━━━━━━━━━━━━━━━━━━ 7157.2s - loss: 0.0005\n",
      "778/1094 ━━━━━━━━━━━━━━━━━━━━ 7165.1s - loss: 0.0005\n",
      "779/1094 ━━━━━━━━━━━━━━━━━━━━ 7172.9s - loss: 0.0005\n",
      "780/1094 ━━━━━━━━━━━━━━━━━━━━ 7180.0s - loss: 0.0005\n",
      "781/1094 ━━━━━━━━━━━━━━━━━━━━ 7187.9s - loss: 0.0005\n",
      "782/1094 ━━━━━━━━━━━━━━━━━━━━ 7195.1s - loss: 0.0005\n",
      "783/1094 ━━━━━━━━━━━━━━━━━━━━ 7203.0s - loss: 0.0005\n",
      "784/1094 ━━━━━━━━━━━━━━━━━━━━ 7209.7s - loss: 0.0005\n",
      "785/1094 ━━━━━━━━━━━━━━━━━━━━ 7217.0s - loss: 0.0005\n",
      "786/1094 ━━━━━━━━━━━━━━━━━━━━ 7224.7s - loss: 0.0005\n",
      "787/1094 ━━━━━━━━━━━━━━━━━━━━ 7232.6s - loss: 0.0005\n",
      "788/1094 ━━━━━━━━━━━━━━━━━━━━ 7239.5s - loss: 0.0005\n",
      "789/1094 ━━━━━━━━━━━━━━━━━━━━ 7246.3s - loss: 0.0005\n",
      "790/1094 ━━━━━━━━━━━━━━━━━━━━ 7253.7s - loss: 0.0005\n",
      "791/1094 ━━━━━━━━━━━━━━━━━━━━ 7261.1s - loss: 0.0005\n",
      "792/1094 ━━━━━━━━━━━━━━━━━━━━ 7268.7s - loss: 0.0005\n",
      "793/1094 ━━━━━━━━━━━━━━━━━━━━ 7275.4s - loss: 0.0005\n",
      "794/1094 ━━━━━━━━━━━━━━━━━━━━ 7284.2s - loss: 0.0005\n",
      "795/1094 ━━━━━━━━━━━━━━━━━━━━ 7291.1s - loss: 0.0005\n",
      "796/1094 ━━━━━━━━━━━━━━━━━━━━ 7298.5s - loss: 0.0005\n",
      "797/1094 ━━━━━━━━━━━━━━━━━━━━ 7305.2s - loss: 0.0005\n",
      "798/1094 ━━━━━━━━━━━━━━━━━━━━ 7312.0s - loss: 0.0005\n",
      "799/1094 ━━━━━━━━━━━━━━━━━━━━ 7318.7s - loss: 0.0005\n",
      "800/1094 ━━━━━━━━━━━━━━━━━━━━ 7326.2s - loss: 0.0005\n",
      "801/1094 ━━━━━━━━━━━━━━━━━━━━ 7333.9s - loss: 0.0005\n",
      "802/1094 ━━━━━━━━━━━━━━━━━━━━ 7341.1s - loss: 0.0005\n",
      "803/1094 ━━━━━━━━━━━━━━━━━━━━ 7348.4s - loss: 0.0005\n",
      "804/1094 ━━━━━━━━━━━━━━━━━━━━ 7355.6s - loss: 0.0005\n",
      "805/1094 ━━━━━━━━━━━━━━━━━━━━ 7363.2s - loss: 0.0005\n",
      "806/1094 ━━━━━━━━━━━━━━━━━━━━ 7369.8s - loss: 0.0005\n",
      "807/1094 ━━━━━━━━━━━━━━━━━━━━ 7377.8s - loss: 0.0005\n",
      "808/1094 ━━━━━━━━━━━━━━━━━━━━ 7384.3s - loss: 0.0005\n",
      "809/1094 ━━━━━━━━━━━━━━━━━━━━ 7392.3s - loss: 0.0005\n",
      "810/1094 ━━━━━━━━━━━━━━━━━━━━ 7399.4s - loss: 0.0005\n",
      "811/1094 ━━━━━━━━━━━━━━━━━━━━ 7406.5s - loss: 0.0005\n",
      "812/1094 ━━━━━━━━━━━━━━━━━━━━ 7414.1s - loss: 0.0005\n",
      "813/1094 ━━━━━━━━━━━━━━━━━━━━ 7420.4s - loss: 0.0005\n",
      "814/1094 ━━━━━━━━━━━━━━━━━━━━ 7429.3s - loss: 0.0005\n",
      "815/1094 ━━━━━━━━━━━━━━━━━━━━ 7436.3s - loss: 0.0005\n",
      "816/1094 ━━━━━━━━━━━━━━━━━━━━ 7443.4s - loss: 0.0005\n",
      "817/1094 ━━━━━━━━━━━━━━━━━━━━ 7450.3s - loss: 0.0005\n",
      "818/1094 ━━━━━━━━━━━━━━━━━━━━ 7458.3s - loss: 0.0005\n",
      "819/1094 ━━━━━━━━━━━━━━━━━━━━ 7465.7s - loss: 0.0005\n",
      "820/1094 ━━━━━━━━━━━━━━━━━━━━ 7473.4s - loss: 0.0005\n",
      "821/1094 ━━━━━━━━━━━━━━━━━━━━ 7480.0s - loss: 0.0005\n",
      "822/1094 ━━━━━━━━━━━━━━━━━━━━ 7486.4s - loss: 0.0005\n",
      "823/1094 ━━━━━━━━━━━━━━━━━━━━ 7494.2s - loss: 0.0005\n",
      "824/1094 ━━━━━━━━━━━━━━━━━━━━ 7500.8s - loss: 0.0005\n",
      "825/1094 ━━━━━━━━━━━━━━━━━━━━ 7508.5s - loss: 0.0005\n",
      "826/1094 ━━━━━━━━━━━━━━━━━━━━ 7514.9s - loss: 0.0005\n",
      "827/1094 ━━━━━━━━━━━━━━━━━━━━ 7522.4s - loss: 0.0005\n",
      "828/1094 ━━━━━━━━━━━━━━━━━━━━ 7529.6s - loss: 0.0005\n",
      "829/1094 ━━━━━━━━━━━━━━━━━━━━ 7537.6s - loss: 0.0005\n",
      "830/1094 ━━━━━━━━━━━━━━━━━━━━ 7544.1s - loss: 0.0005\n",
      "831/1094 ━━━━━━━━━━━━━━━━━━━━ 7552.2s - loss: 0.0005\n",
      "832/1094 ━━━━━━━━━━━━━━━━━━━━ 7559.9s - loss: 0.0005\n",
      "833/1094 ━━━━━━━━━━━━━━━━━━━━ 7567.0s - loss: 0.0005\n",
      "834/1094 ━━━━━━━━━━━━━━━━━━━━ 7573.8s - loss: 0.0005\n",
      "835/1094 ━━━━━━━━━━━━━━━━━━━━ 7581.0s - loss: 0.0005\n",
      "836/1094 ━━━━━━━━━━━━━━━━━━━━ 7588.6s - loss: 0.0005\n",
      "837/1094 ━━━━━━━━━━━━━━━━━━━━ 7595.8s - loss: 0.0005\n",
      "838/1094 ━━━━━━━━━━━━━━━━━━━━ 7603.4s - loss: 0.0005\n",
      "839/1094 ━━━━━━━━━━━━━━━━━━━━ 7610.0s - loss: 0.0005\n",
      "840/1094 ━━━━━━━━━━━━━━━━━━━━ 7617.8s - loss: 0.0005\n",
      "841/1094 ━━━━━━━━━━━━━━━━━━━━ 7624.7s - loss: 0.0005\n",
      "842/1094 ━━━━━━━━━━━━━━━━━━━━ 7631.5s - loss: 0.0005\n",
      "843/1094 ━━━━━━━━━━━━━━━━━━━━ 7638.6s - loss: 0.0005\n",
      "844/1094 ━━━━━━━━━━━━━━━━━━━━ 7645.4s - loss: 0.0005\n",
      "845/1094 ━━━━━━━━━━━━━━━━━━━━ 7653.0s - loss: 0.0005\n",
      "846/1094 ━━━━━━━━━━━━━━━━━━━━ 7660.8s - loss: 0.0005\n",
      "847/1094 ━━━━━━━━━━━━━━━━━━━━ 7669.2s - loss: 0.0005\n",
      "848/1094 ━━━━━━━━━━━━━━━━━━━━ 7675.7s - loss: 0.0005\n",
      "849/1094 ━━━━━━━━━━━━━━━━━━━━ 7682.9s - loss: 0.0005\n",
      "850/1094 ━━━━━━━━━━━━━━━━━━━━ 7690.0s - loss: 0.0005\n",
      "851/1094 ━━━━━━━━━━━━━━━━━━━━ 7697.1s - loss: 0.0005\n",
      "852/1094 ━━━━━━━━━━━━━━━━━━━━ 7703.6s - loss: 0.0005\n",
      "853/1094 ━━━━━━━━━━━━━━━━━━━━ 7710.3s - loss: 0.0005\n",
      "854/1094 ━━━━━━━━━━━━━━━━━━━━ 7717.6s - loss: 0.0004\n",
      "855/1094 ━━━━━━━━━━━━━━━━━━━━ 7725.8s - loss: 0.0004\n",
      "856/1094 ━━━━━━━━━━━━━━━━━━━━ 7734.2s - loss: 0.0004\n",
      "857/1094 ━━━━━━━━━━━━━━━━━━━━ 7740.8s - loss: 0.0004\n",
      "858/1094 ━━━━━━━━━━━━━━━━━━━━ 7748.0s - loss: 0.0005\n",
      "859/1094 ━━━━━━━━━━━━━━━━━━━━ 7755.0s - loss: 0.0004\n",
      "860/1094 ━━━━━━━━━━━━━━━━━━━━ 7763.1s - loss: 0.0004\n",
      "861/1094 ━━━━━━━━━━━━━━━━━━━━ 7769.9s - loss: 0.0004\n",
      "862/1094 ━━━━━━━━━━━━━━━━━━━━ 7777.0s - loss: 0.0004\n",
      "863/1094 ━━━━━━━━━━━━━━━━━━━━ 7784.1s - loss: 0.0004\n",
      "864/1094 ━━━━━━━━━━━━━━━━━━━━ 7791.3s - loss: 0.0004\n",
      "865/1094 ━━━━━━━━━━━━━━━━━━━━ 7798.7s - loss: 0.0004\n",
      "866/1094 ━━━━━━━━━━━━━━━━━━━━ 7805.2s - loss: 0.0004\n",
      "867/1094 ━━━━━━━━━━━━━━━━━━━━ 7813.1s - loss: 0.0004\n",
      "868/1094 ━━━━━━━━━━━━━━━━━━━━ 7820.4s - loss: 0.0004\n",
      "869/1094 ━━━━━━━━━━━━━━━━━━━━ 7827.6s - loss: 0.0004\n",
      "870/1094 ━━━━━━━━━━━━━━━━━━━━ 7834.0s - loss: 0.0004\n",
      "871/1094 ━━━━━━━━━━━━━━━━━━━━ 7841.7s - loss: 0.0004\n",
      "872/1094 ━━━━━━━━━━━━━━━━━━━━ 7848.3s - loss: 0.0004\n",
      "873/1094 ━━━━━━━━━━━━━━━━━━━━ 7856.0s - loss: 0.0004\n",
      "874/1094 ━━━━━━━━━━━━━━━━━━━━ 7863.2s - loss: 0.0004\n",
      "875/1094 ━━━━━━━━━━━━━━━━━━━━ 7870.2s - loss: 0.0004\n",
      "876/1094 ━━━━━━━━━━━━━━━━━━━━ 7877.4s - loss: 0.0004\n",
      "877/1094 ━━━━━━━━━━━━━━━━━━━━ 7884.3s - loss: 0.0004\n",
      "878/1094 ━━━━━━━━━━━━━━━━━━━━ 7891.7s - loss: 0.0004\n",
      "879/1094 ━━━━━━━━━━━━━━━━━━━━ 7898.6s - loss: 0.0004\n",
      "880/1094 ━━━━━━━━━━━━━━━━━━━━ 7906.5s - loss: 0.0004\n",
      "881/1094 ━━━━━━━━━━━━━━━━━━━━ 7912.9s - loss: 0.0004\n",
      "882/1094 ━━━━━━━━━━━━━━━━━━━━ 7920.6s - loss: 0.0004\n",
      "883/1094 ━━━━━━━━━━━━━━━━━━━━ 7928.6s - loss: 0.0004\n",
      "884/1094 ━━━━━━━━━━━━━━━━━━━━ 7935.3s - loss: 0.0004\n",
      "885/1094 ━━━━━━━━━━━━━━━━━━━━ 7942.8s - loss: 0.0004\n",
      "886/1094 ━━━━━━━━━━━━━━━━━━━━ 7949.5s - loss: 0.0004\n",
      "887/1094 ━━━━━━━━━━━━━━━━━━━━ 7956.7s - loss: 0.0004\n",
      "888/1094 ━━━━━━━━━━━━━━━━━━━━ 7963.1s - loss: 0.0004\n",
      "889/1094 ━━━━━━━━━━━━━━━━━━━━ 7970.3s - loss: 0.0004\n",
      "890/1094 ━━━━━━━━━━━━━━━━━━━━ 7977.2s - loss: 0.0004\n",
      "891/1094 ━━━━━━━━━━━━━━━━━━━━ 7985.3s - loss: 0.0004\n",
      "892/1094 ━━━━━━━━━━━━━━━━━━━━ 7992.5s - loss: 0.0004\n",
      "893/1094 ━━━━━━━━━━━━━━━━━━━━ 8000.0s - loss: 0.0004\n",
      "894/1094 ━━━━━━━━━━━━━━━━━━━━ 8007.3s - loss: 0.0004\n",
      "895/1094 ━━━━━━━━━━━━━━━━━━━━ 8014.1s - loss: 0.0004\n",
      "896/1094 ━━━━━━━━━━━━━━━━━━━━ 8021.7s - loss: 0.0004\n",
      "897/1094 ━━━━━━━━━━━━━━━━━━━━ 8029.7s - loss: 0.0004\n",
      "898/1094 ━━━━━━━━━━━━━━━━━━━━ 8037.5s - loss: 0.0004\n",
      "899/1094 ━━━━━━━━━━━━━━━━━━━━ 8044.6s - loss: 0.0004\n",
      "900/1094 ━━━━━━━━━━━━━━━━━━━━ 8053.2s - loss: 0.0004\n",
      "901/1094 ━━━━━━━━━━━━━━━━━━━━ 8062.6s - loss: 0.0004\n",
      "902/1094 ━━━━━━━━━━━━━━━━━━━━ 8070.6s - loss: 0.0004\n",
      "903/1094 ━━━━━━━━━━━━━━━━━━━━ 8077.2s - loss: 0.0004\n",
      "904/1094 ━━━━━━━━━━━━━━━━━━━━ 8084.6s - loss: 0.0004\n",
      "905/1094 ━━━━━━━━━━━━━━━━━━━━ 8091.9s - loss: 0.0004\n",
      "906/1094 ━━━━━━━━━━━━━━━━━━━━ 8099.1s - loss: 0.0004\n",
      "907/1094 ━━━━━━━━━━━━━━━━━━━━ 8105.6s - loss: 0.0004\n",
      "908/1094 ━━━━━━━━━━━━━━━━━━━━ 8113.1s - loss: 0.0004\n",
      "909/1094 ━━━━━━━━━━━━━━━━━━━━ 8120.3s - loss: 0.0004\n",
      "910/1094 ━━━━━━━━━━━━━━━━━━━━ 8129.9s - loss: 0.0004\n",
      "911/1094 ━━━━━━━━━━━━━━━━━━━━ 8136.8s - loss: 0.0004\n",
      "912/1094 ━━━━━━━━━━━━━━━━━━━━ 8143.4s - loss: 0.0004\n",
      "913/1094 ━━━━━━━━━━━━━━━━━━━━ 8151.0s - loss: 0.0004\n",
      "914/1094 ━━━━━━━━━━━━━━━━━━━━ 8157.5s - loss: 0.0004\n",
      "915/1094 ━━━━━━━━━━━━━━━━━━━━ 8165.1s - loss: 0.0004\n",
      "916/1094 ━━━━━━━━━━━━━━━━━━━━ 8171.8s - loss: 0.0004\n",
      "917/1094 ━━━━━━━━━━━━━━━━━━━━ 8179.2s - loss: 0.0004\n",
      "918/1094 ━━━━━━━━━━━━━━━━━━━━ 8186.1s - loss: 0.0004\n",
      "919/1094 ━━━━━━━━━━━━━━━━━━━━ 8193.8s - loss: 0.0004\n",
      "920/1094 ━━━━━━━━━━━━━━━━━━━━ 8201.2s - loss: 0.0004\n",
      "921/1094 ━━━━━━━━━━━━━━━━━━━━ 8208.2s - loss: 0.0004\n",
      "922/1094 ━━━━━━━━━━━━━━━━━━━━ 8215.8s - loss: 0.0004\n",
      "923/1094 ━━━━━━━━━━━━━━━━━━━━ 8222.5s - loss: 0.0004\n",
      "924/1094 ━━━━━━━━━━━━━━━━━━━━ 8229.5s - loss: 0.0004\n",
      "925/1094 ━━━━━━━━━━━━━━━━━━━━ 8236.3s - loss: 0.0004\n",
      "926/1094 ━━━━━━━━━━━━━━━━━━━━ 8244.2s - loss: 0.0004\n",
      "927/1094 ━━━━━━━━━━━━━━━━━━━━ 8251.4s - loss: 0.0004\n",
      "928/1094 ━━━━━━━━━━━━━━━━━━━━ 8259.4s - loss: 0.0004\n",
      "929/1094 ━━━━━━━━━━━━━━━━━━━━ 8266.6s - loss: 0.0004\n",
      "930/1094 ━━━━━━━━━━━━━━━━━━━━ 8274.0s - loss: 0.0004\n",
      "931/1094 ━━━━━━━━━━━━━━━━━━━━ 8280.9s - loss: 0.0004\n",
      "932/1094 ━━━━━━━━━━━━━━━━━━━━ 8289.2s - loss: 0.0004\n",
      "933/1094 ━━━━━━━━━━━━━━━━━━━━ 8296.3s - loss: 0.0004\n",
      "934/1094 ━━━━━━━━━━━━━━━━━━━━ 8303.6s - loss: 0.0004\n",
      "935/1094 ━━━━━━━━━━━━━━━━━━━━ 8311.0s - loss: 0.0004\n",
      "936/1094 ━━━━━━━━━━━━━━━━━━━━ 8318.7s - loss: 0.0004\n",
      "937/1094 ━━━━━━━━━━━━━━━━━━━━ 8325.9s - loss: 0.0004\n",
      "938/1094 ━━━━━━━━━━━━━━━━━━━━ 8332.5s - loss: 0.0004\n",
      "939/1094 ━━━━━━━━━━━━━━━━━━━━ 8339.7s - loss: 0.0004\n",
      "940/1094 ━━━━━━━━━━━━━━━━━━━━ 8346.9s - loss: 0.0004\n",
      "941/1094 ━━━━━━━━━━━━━━━━━━━━ 8354.5s - loss: 0.0004\n",
      "942/1094 ━━━━━━━━━━━━━━━━━━━━ 8362.0s - loss: 0.0004\n",
      "943/1094 ━━━━━━━━━━━━━━━━━━━━ 8369.5s - loss: 0.0004\n",
      "944/1094 ━━━━━━━━━━━━━━━━━━━━ 8376.7s - loss: 0.0004\n",
      "945/1094 ━━━━━━━━━━━━━━━━━━━━ 8386.0s - loss: 0.0004\n",
      "946/1094 ━━━━━━━━━━━━━━━━━━━━ 8393.5s - loss: 0.0004\n",
      "947/1094 ━━━━━━━━━━━━━━━━━━━━ 8401.3s - loss: 0.0004\n",
      "948/1094 ━━━━━━━━━━━━━━━━━━━━ 8407.8s - loss: 0.0004\n",
      "949/1094 ━━━━━━━━━━━━━━━━━━━━ 8416.4s - loss: 0.0004\n",
      "950/1094 ━━━━━━━━━━━━━━━━━━━━ 8423.7s - loss: 0.0004\n",
      "951/1094 ━━━━━━━━━━━━━━━━━━━━ 8430.6s - loss: 0.0004\n",
      "952/1094 ━━━━━━━━━━━━━━━━━━━━ 8438.5s - loss: 0.0004\n",
      "953/1094 ━━━━━━━━━━━━━━━━━━━━ 8445.9s - loss: 0.0004\n",
      "954/1094 ━━━━━━━━━━━━━━━━━━━━ 8453.9s - loss: 0.0004\n",
      "955/1094 ━━━━━━━━━━━━━━━━━━━━ 8461.2s - loss: 0.0004\n",
      "956/1094 ━━━━━━━━━━━━━━━━━━━━ 8468.4s - loss: 0.0004\n",
      "957/1094 ━━━━━━━━━━━━━━━━━━━━ 8476.7s - loss: 0.0004\n",
      "958/1094 ━━━━━━━━━━━━━━━━━━━━ 8485.0s - loss: 0.0004\n",
      "959/1094 ━━━━━━━━━━━━━━━━━━━━ 8491.8s - loss: 0.0004\n",
      "960/1094 ━━━━━━━━━━━━━━━━━━━━ 8499.2s - loss: 0.0004\n",
      "961/1094 ━━━━━━━━━━━━━━━━━━━━ 8506.6s - loss: 0.0004\n",
      "962/1094 ━━━━━━━━━━━━━━━━━━━━ 8514.0s - loss: 0.0004\n",
      "963/1094 ━━━━━━━━━━━━━━━━━━━━ 8521.4s - loss: 0.0004\n",
      "964/1094 ━━━━━━━━━━━━━━━━━━━━ 8528.9s - loss: 0.0004\n",
      "965/1094 ━━━━━━━━━━━━━━━━━━━━ 8535.7s - loss: 0.0004\n",
      "966/1094 ━━━━━━━━━━━━━━━━━━━━ 8543.2s - loss: 0.0004\n",
      "967/1094 ━━━━━━━━━━━━━━━━━━━━ 8550.2s - loss: 0.0004\n",
      "968/1094 ━━━━━━━━━━━━━━━━━━━━ 8556.7s - loss: 0.0004\n",
      "969/1094 ━━━━━━━━━━━━━━━━━━━━ 8564.5s - loss: 0.0004\n",
      "970/1094 ━━━━━━━━━━━━━━━━━━━━ 8571.7s - loss: 0.0004\n",
      "971/1094 ━━━━━━━━━━━━━━━━━━━━ 8579.7s - loss: 0.0004\n",
      "972/1094 ━━━━━━━━━━━━━━━━━━━━ 8586.9s - loss: 0.0004\n",
      "973/1094 ━━━━━━━━━━━━━━━━━━━━ 8594.2s - loss: 0.0004\n",
      "974/1094 ━━━━━━━━━━━━━━━━━━━━ 8601.3s - loss: 0.0004\n",
      "975/1094 ━━━━━━━━━━━━━━━━━━━━ 8610.2s - loss: 0.0004\n",
      "976/1094 ━━━━━━━━━━━━━━━━━━━━ 8617.4s - loss: 0.0004\n",
      "977/1094 ━━━━━━━━━━━━━━━━━━━━ 8624.9s - loss: 0.0004\n",
      "978/1094 ━━━━━━━━━━━━━━━━━━━━ 8631.6s - loss: 0.0004\n",
      "979/1094 ━━━━━━━━━━━━━━━━━━━━ 8638.7s - loss: 0.0004\n",
      "980/1094 ━━━━━━━━━━━━━━━━━━━━ 8646.4s - loss: 0.0004\n",
      "981/1094 ━━━━━━━━━━━━━━━━━━━━ 8653.3s - loss: 0.0004\n",
      "982/1094 ━━━━━━━━━━━━━━━━━━━━ 8660.7s - loss: 0.0004\n",
      "983/1094 ━━━━━━━━━━━━━━━━━━━━ 8667.6s - loss: 0.0004\n",
      "984/1094 ━━━━━━━━━━━━━━━━━━━━ 8674.8s - loss: 0.0004\n",
      "985/1094 ━━━━━━━━━━━━━━━━━━━━ 8681.8s - loss: 0.0004\n",
      "986/1094 ━━━━━━━━━━━━━━━━━━━━ 8689.1s - loss: 0.0004\n",
      "987/1094 ━━━━━━━━━━━━━━━━━━━━ 8696.6s - loss: 0.0004\n",
      "988/1094 ━━━━━━━━━━━━━━━━━━━━ 8704.9s - loss: 0.0004\n",
      "989/1094 ━━━━━━━━━━━━━━━━━━━━ 8712.7s - loss: 0.0004\n",
      "990/1094 ━━━━━━━━━━━━━━━━━━━━ 8720.9s - loss: 0.0004\n",
      "991/1094 ━━━━━━━━━━━━━━━━━━━━ 8727.8s - loss: 0.0004\n",
      "992/1094 ━━━━━━━━━━━━━━━━━━━━ 8734.9s - loss: 0.0004\n",
      "993/1094 ━━━━━━━━━━━━━━━━━━━━ 8742.1s - loss: 0.0004\n",
      "994/1094 ━━━━━━━━━━━━━━━━━━━━ 8748.9s - loss: 0.0004\n",
      "995/1094 ━━━━━━━━━━━━━━━━━━━━ 8756.3s - loss: 0.0004\n",
      "996/1094 ━━━━━━━━━━━━━━━━━━━━ 8763.2s - loss: 0.0004\n",
      "997/1094 ━━━━━━━━━━━━━━━━━━━━ 8770.6s - loss: 0.0004\n",
      "998/1094 ━━━━━━━━━━━━━━━━━━━━ 8779.4s - loss: 0.0004\n",
      "999/1094 ━━━━━━━━━━━━━━━━━━━━ 8787.9s - loss: 0.0004\n",
      "1000/1094 ━━━━━━━━━━━━━━━━━━━━ 8794.3s - loss: 0.0004\n",
      "1001/1094 ━━━━━━━━━━━━━━━━━━━━ 8801.8s - loss: 0.0004\n",
      "1002/1094 ━━━━━━━━━━━━━━━━━━━━ 8808.4s - loss: 0.0004\n",
      "1003/1094 ━━━━━━━━━━━━━━━━━━━━ 8815.7s - loss: 0.0004\n",
      "1004/1094 ━━━━━━━━━━━━━━━━━━━━ 8823.0s - loss: 0.0004\n",
      "1005/1094 ━━━━━━━━━━━━━━━━━━━━ 8829.9s - loss: 0.0004\n",
      "1006/1094 ━━━━━━━━━━━━━━━━━━━━ 8837.5s - loss: 0.0004\n",
      "1007/1094 ━━━━━━━━━━━━━━━━━━━━ 8844.7s - loss: 0.0004\n",
      "1008/1094 ━━━━━━━━━━━━━━━━━━━━ 8852.5s - loss: 0.0004\n",
      "1009/1094 ━━━━━━━━━━━━━━━━━━━━ 8859.2s - loss: 0.0004\n",
      "1010/1094 ━━━━━━━━━━━━━━━━━━━━ 8866.7s - loss: 0.0003\n",
      "1011/1094 ━━━━━━━━━━━━━━━━━━━━ 8873.6s - loss: 0.0003\n",
      "1012/1094 ━━━━━━━━━━━━━━━━━━━━ 8880.7s - loss: 0.0004\n",
      "1013/1094 ━━━━━━━━━━━━━━━━━━━━ 8887.6s - loss: 0.0004\n",
      "1014/1094 ━━━━━━━━━━━━━━━━━━━━ 8894.1s - loss: 0.0003\n",
      "1015/1094 ━━━━━━━━━━━━━━━━━━━━ 8901.4s - loss: 0.0004\n",
      "1016/1094 ━━━━━━━━━━━━━━━━━━━━ 8908.4s - loss: 0.0003\n",
      "1017/1094 ━━━━━━━━━━━━━━━━━━━━ 8916.1s - loss: 0.0004\n",
      "1018/1094 ━━━━━━━━━━━━━━━━━━━━ 8923.0s - loss: 0.0004\n",
      "1019/1094 ━━━━━━━━━━━━━━━━━━━━ 8930.3s - loss: 0.0004\n",
      "1020/1094 ━━━━━━━━━━━━━━━━━━━━ 8937.4s - loss: 0.0003\n",
      "1021/1094 ━━━━━━━━━━━━━━━━━━━━ 8946.2s - loss: 0.0003\n",
      "1022/1094 ━━━━━━━━━━━━━━━━━━━━ 8953.4s - loss: 0.0003\n",
      "1023/1094 ━━━━━━━━━━━━━━━━━━━━ 8961.1s - loss: 0.0003\n",
      "1024/1094 ━━━━━━━━━━━━━━━━━━━━ 8968.3s - loss: 0.0003\n",
      "1025/1094 ━━━━━━━━━━━━━━━━━━━━ 8975.5s - loss: 0.0003\n",
      "1026/1094 ━━━━━━━━━━━━━━━━━━━━ 8983.1s - loss: 0.0003\n",
      "1027/1094 ━━━━━━━━━━━━━━━━━━━━ 8989.8s - loss: 0.0003\n",
      "1028/1094 ━━━━━━━━━━━━━━━━━━━━ 8997.6s - loss: 0.0003\n",
      "1029/1094 ━━━━━━━━━━━━━━━━━━━━ 9004.7s - loss: 0.0003\n",
      "1030/1094 ━━━━━━━━━━━━━━━━━━━━ 9012.2s - loss: 0.0003\n",
      "1031/1094 ━━━━━━━━━━━━━━━━━━━━ 9019.1s - loss: 0.0003\n",
      "1032/1094 ━━━━━━━━━━━━━━━━━━━━ 9026.4s - loss: 0.0003\n",
      "1033/1094 ━━━━━━━━━━━━━━━━━━━━ 9034.0s - loss: 0.0003\n",
      "1034/1094 ━━━━━━━━━━━━━━━━━━━━ 9042.1s - loss: 0.0003\n",
      "1035/1094 ━━━━━━━━━━━━━━━━━━━━ 9049.3s - loss: 0.0003\n",
      "1036/1094 ━━━━━━━━━━━━━━━━━━━━ 9056.7s - loss: 0.0003\n",
      "1037/1094 ━━━━━━━━━━━━━━━━━━━━ 9063.5s - loss: 0.0003\n",
      "1038/1094 ━━━━━━━━━━━━━━━━━━━━ 9072.3s - loss: 0.0003\n",
      "1039/1094 ━━━━━━━━━━━━━━━━━━━━ 9079.8s - loss: 0.0003\n",
      "1040/1094 ━━━━━━━━━━━━━━━━━━━━ 9086.5s - loss: 0.0003\n",
      "1041/1094 ━━━━━━━━━━━━━━━━━━━━ 9093.7s - loss: 0.0003\n",
      "1042/1094 ━━━━━━━━━━━━━━━━━━━━ 9101.3s - loss: 0.0003\n",
      "1043/1094 ━━━━━━━━━━━━━━━━━━━━ 9108.9s - loss: 0.0003\n",
      "1044/1094 ━━━━━━━━━━━━━━━━━━━━ 9115.9s - loss: 0.0003\n",
      "1045/1094 ━━━━━━━━━━━━━━━━━━━━ 9123.2s - loss: 0.0003\n",
      "1046/1094 ━━━━━━━━━━━━━━━━━━━━ 9130.2s - loss: 0.0003\n",
      "1047/1094 ━━━━━━━━━━━━━━━━━━━━ 9137.5s - loss: 0.0003\n",
      "1048/1094 ━━━━━━━━━━━━━━━━━━━━ 9144.2s - loss: 0.0003\n",
      "1049/1094 ━━━━━━━━━━━━━━━━━━━━ 9152.7s - loss: 0.0003\n",
      "1050/1094 ━━━━━━━━━━━━━━━━━━━━ 9160.1s - loss: 0.0003\n",
      "1051/1094 ━━━━━━━━━━━━━━━━━━━━ 9167.6s - loss: 0.0003\n",
      "1052/1094 ━━━━━━━━━━━━━━━━━━━━ 9175.5s - loss: 0.0003\n",
      "1053/1094 ━━━━━━━━━━━━━━━━━━━━ 9183.7s - loss: 0.0003\n",
      "1054/1094 ━━━━━━━━━━━━━━━━━━━━ 9191.6s - loss: 0.0003\n",
      "1055/1094 ━━━━━━━━━━━━━━━━━━━━ 9199.2s - loss: 0.0003\n",
      "1056/1094 ━━━━━━━━━━━━━━━━━━━━ 9207.0s - loss: 0.0003\n",
      "1057/1094 ━━━━━━━━━━━━━━━━━━━━ 9213.5s - loss: 0.0003\n",
      "1058/1094 ━━━━━━━━━━━━━━━━━━━━ 9220.5s - loss: 0.0003\n",
      "1059/1094 ━━━━━━━━━━━━━━━━━━━━ 9227.1s - loss: 0.0003\n",
      "1060/1094 ━━━━━━━━━━━━━━━━━━━━ 9235.0s - loss: 0.0003\n",
      "1061/1094 ━━━━━━━━━━━━━━━━━━━━ 9242.7s - loss: 0.0003\n",
      "1062/1094 ━━━━━━━━━━━━━━━━━━━━ 9250.3s - loss: 0.0003\n",
      "1063/1094 ━━━━━━━━━━━━━━━━━━━━ 9256.5s - loss: 0.0003\n",
      "1064/1094 ━━━━━━━━━━━━━━━━━━━━ 9263.3s - loss: 0.0003\n",
      "1065/1094 ━━━━━━━━━━━━━━━━━━━━ 9270.6s - loss: 0.0003\n",
      "1066/1094 ━━━━━━━━━━━━━━━━━━━━ 9276.9s - loss: 0.0003\n",
      "1067/1094 ━━━━━━━━━━━━━━━━━━━━ 9283.8s - loss: 0.0003\n",
      "1068/1094 ━━━━━━━━━━━━━━━━━━━━ 9290.0s - loss: 0.0003\n",
      "1069/1094 ━━━━━━━━━━━━━━━━━━━━ 9297.4s - loss: 0.0003\n",
      "1070/1094 ━━━━━━━━━━━━━━━━━━━━ 9304.1s - loss: 0.0003\n",
      "1071/1094 ━━━━━━━━━━━━━━━━━━━━ 9310.8s - loss: 0.0003\n",
      "1072/1094 ━━━━━━━━━━━━━━━━━━━━ 9317.7s - loss: 0.0003\n",
      "1073/1094 ━━━━━━━━━━━━━━━━━━━━ 9323.6s - loss: 0.0003\n",
      "1074/1094 ━━━━━━━━━━━━━━━━━━━━ 9330.7s - loss: 0.0003\n",
      "1075/1094 ━━━━━━━━━━━━━━━━━━━━ 9337.9s - loss: 0.0003\n",
      "1076/1094 ━━━━━━━━━━━━━━━━━━━━ 9344.9s - loss: 0.0003\n",
      "1077/1094 ━━━━━━━━━━━━━━━━━━━━ 9351.2s - loss: 0.0003\n",
      "1078/1094 ━━━━━━━━━━━━━━━━━━━━ 9357.4s - loss: 0.0003\n",
      "1079/1094 ━━━━━━━━━━━━━━━━━━━━ 9364.7s - loss: 0.0003\n",
      "1080/1094 ━━━━━━━━━━━━━━━━━━━━ 9371.5s - loss: 0.0003\n",
      "1081/1094 ━━━━━━━━━━━━━━━━━━━━ 9379.3s - loss: 0.0003\n",
      "1082/1094 ━━━━━━━━━━━━━━━━━━━━ 9385.6s - loss: 0.0003\n",
      "1083/1094 ━━━━━━━━━━━━━━━━━━━━ 9392.2s - loss: 0.0003\n",
      "1084/1094 ━━━━━━━━━━━━━━━━━━━━ 9398.7s - loss: 0.0003\n",
      "1085/1094 ━━━━━━━━━━━━━━━━━━━━ 9405.4s - loss: 0.0003\n",
      "1086/1094 ━━━━━━━━━━━━━━━━━━━━ 9412.1s - loss: 0.0003\n",
      "1087/1094 ━━━━━━━━━━━━━━━━━━━━ 9418.1s - loss: 0.0003\n",
      "1088/1094 ━━━━━━━━━━━━━━━━━━━━ 9424.7s - loss: 0.0003\n",
      "1089/1094 ━━━━━━━━━━━━━━━━━━━━ 9431.5s - loss: 0.0003\n",
      "1090/1094 ━━━━━━━━━━━━━━━━━━━━ 9438.5s - loss: 0.0003\n",
      "1091/1094 ━━━━━━━━━━━━━━━━━━━━ 9445.8s - loss: 0.0003\n",
      "1092/1094 ━━━━━━━━━━━━━━━━━━━━ 9452.0s - loss: 0.0003\n",
      "1093/1094 ━━━━━━━━━━━━━━━━━━━━ 9458.9s - loss: 0.0003\n",
      "1094/1094 ━━━━━━━━━━━━━━━━━━━━ 9463.8s - loss: 0.0003\n",
      "Epoch 1/1 - Loss: 0.0090\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "# os: Библиотека для работы с операционной системой, позволяет взаимодействовать с файловой системой.\n",
    "# json: Библиотека для работы с JSON-файлами.\n",
    "# time: Библиотека для работы с временем, используется для измерения времени выполнения.\n",
    "# torch: Основная библиотека для работы с глубоким обучением от PyTorch.\n",
    "# Dataset, DataLoader: Классы из PyTorch для работы с наборами данных и их загрузкой.\n",
    "# BertTokenizer, BertModel: Классы из библиотеки transformers, которые используются для обработки текста с помощью модели BERT\n",
    "\n",
    "class CLEVRDataset(Dataset): # класс CLEVRDataset будет использоваться для загрузки и обработки данных из набора данных CLEVR\n",
    "    def __init__(self, data_dir, split='train'): # data_dir: путь к директории с данными, split указывает, какую часть данных загружать (по умолчанию 'train')\n",
    "        self.data_dir = data_dir # сохранение переданного пути к папке с данными (data_dir) в переменной экземпляра self.data_dir\n",
    "        self.scenes, self.images = self.load_data(split) # вызов метода load_data, передача ему параметр split\n",
    "        # scenes: это данные о сценах\n",
    "        # images: это индексы изображений, связанных с этими сценами\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # создание токенизатора, с использованием модели BERT\n",
    "        # которая называется 'bert-base-uncased'\n",
    "        # Токенизатор — это инструмент, который разбивает текст на части (токены), чтобы модель могла его понять\n",
    "\n",
    "    def load_data(self, split): # метод, который загружает данные из файлов\n",
    "        # split: Это часть данных, которая загружается ('train', 'val' или 'test')\n",
    "        scenes_path = os.path.join(self.data_dir, 'scenes', f'CLEVR_{split}_scenes.json') # self.data_dir — это папка с данными\n",
    "        # 'scenes' — это подкаталог, где находятся файлы с данными\n",
    "        # f'CLEVR_{split}_scenes.json' — это название файла, которое зависит от значения параметра split\n",
    "\n",
    "        if not os.path.isfile(scenes_path): # проверка существования файла по указанному пути scenes_path\n",
    "            raise FileNotFoundError(f\"Файл не найден: {scenes_path}\")\n",
    "\n",
    "        with open(scenes_path) as f: # используя json.load(f), мы загружаем данные из файла в формате JSON\n",
    "            scenes_data = json.load(f)['scenes'] # извлечение части данных,  под ключом 'scenes', и сохранение в переменной scenes_data\n",
    "        \n",
    "        scenes = [] # scenes: для хранения данных о сценах\n",
    "        images = [] # images: для хранения индексов изображений, связанных с этими сценами\n",
    "\n",
    "        for scene in scenes_data: # цикл по каждой сцене в scenes_data\n",
    "            scenes.append(scene) # добавление в список scenes\n",
    "            images.append(scene.get('image_index', None)) # # извлечение индекса изображения с помощью scene.get('image_index', None)\n",
    "            # Если ключ 'image_index' не найден, добавление None\n",
    "\n",
    "        return scenes, images # возвращение двух списков: scenes и images\n",
    "\n",
    "    def __len__(self): # определяет, сколько элементов (сцен) содержится в наборе данных\n",
    "        return len(self.scenes) # возвращает длину списка self.scenes с помощью метода load_data\n",
    "        # позволит использовать встроенные функции, такие как len(), для получения количества сцен в наборе\n",
    "\n",
    "    def __getitem__(self, idx): # метод позволяет получать элементы из набора данных по индексу\n",
    "        # idx: индекс элемента, который нужно получить\n",
    "        scene = self.scenes[idx] # idx, чтобы получить соответствующую сцену из списка self.scenes\n",
    "        image_id = self.images[idx] # индекс изображения из списка self.images\n",
    "        description = self.get_scene_description(scene) # вызов метод get_scene_description, передача ему текущей сцены\n",
    "        return description, image_id # возвращаем description: текстовое описание сцены, image_id: индекс изображения, связанного с этой сценой\n",
    "\n",
    "    def get_scene_description(self, scene): # scene:словарь, представляющий сцену, которая содержит информацию о различных объектах в ней\n",
    "        # метод get для получения списка объектов из сцены\n",
    "        # ключ 'objects' отсутствует, мы возвращаем пустой список []. Это предотвращает возникновение ошибок, если в сцене нет объектов.\n",
    "        return \" \".join([f\"{obj['color']} {obj['shape']}\" for obj in scene.get('objects', [])]) # для каждого объекта obj в списке объектов \n",
    "        # формируется строка, которая состоит из цвета и формы объекта\n",
    "\n",
    "class SimpleTransformerModel(torch.nn.Module): # класс SimpleTransformerModel от torch.nn.Module, \n",
    "    # позволяет использовать функциональность PyTorch для создания нейронных сетей\n",
    "    def __init__(self): # __init__ является конструктором класса, который вызывается при создании нового объекта класса\n",
    "        super(SimpleTransformerModel, self).__init__() \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased') # загрузка предобученной модели BERT с помощью метода from_pretrained# \n",
    "        self.fc = torch.nn.Linear(self.bert.config.hidden_size, 1) # модель bert-base-uncased — это версия BERT, которая не учитывает регистр\n",
    "# создание линейного слоя (fc), который будет использоваться для преобразования выходных данных BERT в выходное значение\n",
    "# self.bert.config.hidden_size — это размер скрытого состояния выходного вектора BERT \n",
    "# 1 указывает на то, что выходной слой будет возвращать одно значение\n",
    "\n",
    "    def forward(self, input_ids, attention_mask): # input_ids: тензор, представляющий последовательности токенов, которые будут переданы в модель BERT\n",
    "    # attention_mask: Это тензор, который используется для указания модели, какие токены должны быть обработаны (1) и какие игнорироваться (0)\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask) # входные данные (input_ids и attention_mask) передаются в предобученную модель BERT\n",
    "# метод возвращает объект outputs, который содержит различные выходные данные модели, включая скрытые состояния и выход для пула\n",
    "        pooled_output = outputs.pooler_output # outputs.pooler_output — это выходное значение, полученное после применения слоя пула к выходам BERT\n",
    "        return self.fc(pooled_output) # после получения pooled_output, он передается в линейный слой self.fc\n",
    "        # линейный слой преобразует это представление в одно значение, которое будет являться выходом модели\n",
    "\n",
    "    def summary(self): # \n",
    "        \n",
    "        print(\"Model Summary:\") # выводит заголовок\n",
    "        print(f\"{'Layer':<30} {'Output Shape':<25} {'Param #':<15}\") # \n",
    "# Форматирует и выводит заголовки для таблицы:\n",
    "# 'Layer' — название слоя,\n",
    "# 'Output Shape' — форма выходных данных слоя,\n",
    "# 'Param #' — количество параметров в слое\n",
    "        \n",
    "        print(\"=\" * 70) # выводит горизонтальную линию\n",
    "        \n",
    "        for name, param in self.named_parameters(): # Цикл, который перебирает все параметры модели\n",
    "            \n",
    "            output_shape = str(param.shape) # получение формы параметра и преобразование ее в строку для вывода\n",
    "            print(f\"{name:<30} {output_shape:<25} {param.numel():<15}\") # вывод имя параметра, его форму и количество элементов\n",
    "        print(\"=\" * 70) # горизонтальная линия\n",
    "        total_params = sum(p.numel() for p in self.parameters()) # генераторное выражение для подсчета общего количества параметров в модели\n",
    "        # вызывая метод numel() для каждого параметра\n",
    "        print(f\"Total params: {total_params}\") # вывод общего количество параметров в модели\n",
    "\n",
    "def prepare_data(data_dir, split='train', batch_size=64): # data_dir: директория, в которой находятся данные\n",
    "    # split: строка, указывающая, какое разбиение данных использовать\n",
    "    # batch_size: размер пакета, который будет использоваться при загрузке данных\n",
    "    dataset = CLEVRDataset(data_dir, split) # создание экземпляра класса CLEVRDataset, \n",
    "    # который отвечает за загрузку и предобработку данных из указанной директории для данного разбиения\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # создается объект DataLoader, который будет использоваться для итерации по датасету\n",
    "    # batch_size=batch_size указывает размер пакета\n",
    "    # shuffle=True означает, что данные будут перемешаны перед каждой эпохой, что помогает улучшить обобщающую способность модели\n",
    "    return dataloader # функция возвращает созданный загрузчик данных\n",
    "\n",
    "def train_model(model, dataloader, epochs=1): # model: модель, которую нужно обучить\n",
    "    # dataloader: загрузчик данных, который предоставляет данные для обучения\n",
    "    # epochs: количество эпох для обучения\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5) # оптимизатор Adam с заданной скоростью обучения\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss() # функция потерь BCEWithLogitsLoss используется для \n",
    "    # бинарной классификации, комбинируя сигмоид и бинарную кросс-энтропию в одном шаге\n",
    "\n",
    "    model.train() # устанавливает модель в режим обучения, что включает поведение, специфичное для обучения\n",
    "    total_steps = len(dataloader) # определяет общее количество шагов (батчей) в одной эпохе\n",
    "    \n",
    "    for epoch in range(epochs): # цикл по эпохам\n",
    "        epoch_loss = 0.0 # инициализация переменных для отслеживания потерь и правильных предсказаний\n",
    "        correct_predictions = 0 \n",
    "        \n",
    "        start_time = time.time() # запуск таймера\n",
    "        \n",
    "        for step, (descriptions, _) in enumerate(dataloader): # перебирает загрузчик данных, получая описания и метки\n",
    "            encoding = dataloader.dataset.tokenizer(descriptions, return_tensors='pt', padding=True, truncation=True)\n",
    "            # токенизирует описания, возвращая тензоры для входных данных\n",
    "\n",
    "            input_ids = encoding['input_ids'] # получение входных идентификаторов и маски внимания\n",
    "            attention_mask = encoding['attention_mask'] # \n",
    "            labels = torch.zeros(input_ids.size(0), 1)  # создает нулевые метки для обучения\n",
    "\n",
    "            optimizer.zero_grad() # обнуление градиентов\n",
    "            outputs = model(input_ids, attention_mask) # прямой проход через модель\n",
    "            loss = loss_fn(outputs, labels) # вычисление потерь\n",
    "            loss.backward() # обратное распространение ошибки и шаг оптимизации\n",
    "            optimizer.step() # \n",
    "\n",
    "            epoch_loss += loss.item() # суммирование потерь за эпоху\n",
    "\n",
    "            if step % 1 == 0: # это условие всегда истинно, так как step % 1 всегда будет равно 0\n",
    "                # Таким образом, информация о ходе обучения будет выводиться на каждой итерации\n",
    "                elapsed_time = time.time() - start_time # вычисляется время, прошедшее с начала текущей эпохи, используя time.time()\n",
    "                print(f\"{step + 1}/{total_steps}{elapsed_time:.1f}s - loss: {loss.item():.4f}\") \n",
    "# Номер текущего шага (step + 1), чтобы начинать с 1 вместо 0\n",
    "# Общее количество шагов в эпохе (total_steps)\n",
    "# Время, прошедшее с начала эпохи (elapsed_time), округленное до одного знака после запятой\n",
    "# Текущую потерю (loss.item()), округленную до четырех знаков после запятой\n",
    "\n",
    "        avg_loss = epoch_loss / total_steps\n",
    "# После завершения всех шагов в эпохе, вычисляется средняя потеря, деля общую потерю за эпоху (epoch_loss) на количество шагов (total_steps)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {avg_loss:.4f}\") # выводится средняя потеря для текущей эпохи\n",
    "\n",
    "def predict(model, description):\n",
    "    model.eval() # модель в режим оценки, что отключает такие механизмы, как дропаут, которые могут влиять на предсказания\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "    # токенизатор BERT, который будет использоваться для преобразования текстового описания\n",
    "    encoding = tokenizer(description, return_tensors='pt', padding=True, truncation=True)\n",
    "# Токенизирует входное описание, возвращая тензоры для входных идентификаторов и маски внимания\n",
    "# Параметры padding=True и truncation=True обеспечивают правильное форматирование входных данных. \n",
    "\n",
    "    with torch.no_grad(): # для отключения вычисления градиентов\n",
    "        input_ids = encoding['input_ids'] # получение идентификаторов и маски внимания\n",
    "        attention_mask = encoding['attention_mask'] \n",
    "        output = model(input_ids, attention_mask) # получение предсказания\n",
    "        \n",
    "    return output # возврат предсказания\n",
    "\n",
    "if __name__ == \"__main__\": # блок кода выполняется только тогда, когда скрипт запускается напрямую, а не импортируется как модуль\n",
    "    data_dir = \"C:/Users/Dasha/Desktop/CLEVR_v1.0\" # data_dir: указывает на местоположение данных\n",
    "    batch_size = 64 # batch_size: определяет количество образцов, обрабатываемых одновременно\n",
    "    epochs = 1 # epochs: указывает, сколько раз модель будет проходить через весь набор данных\n",
    "\n",
    "    dataloader = prepare_data(data_dir, split='train', batch_size=batch_size) # prepare_data(...): эта функция загружает и обрабатывает данные, \n",
    "# возвращая объект dataloader, который будет использоваться для итерации по данным во время обучения\n",
    "\n",
    "    model = SimpleTransformerModel() # SimpleTransformerModel(): Создает экземпляр модели\n",
    "\n",
    "    # Выводим сводку модели до обучения\n",
    "    model.summary() # model.summary(): функция выводит информацию о структуре модели\n",
    "\n",
    "    train_model(model, dataloader, epochs=epochs) # train_model(...):функция запускает процесс обучения модели, \n",
    "    # используя подготовленные данные и заданное количество эпох\n",
    "\n",
    "    # Пример предсказания\n",
    "    # description = \"Where is the red cube?\"\n",
    "    # prediction = predict(model, description)\n",
    "    # print(f\"Prediction for description '{description}': {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b8e38-42c7-4a15-bb68-aacc9d866c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
