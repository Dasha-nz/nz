{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f9bcc42-08b1-4262-990b-07fc732a62cf",
   "metadata": {},
   "source": [
    "import tensorflow as tf #библиотека с открытым исходным кодом для машинного обучения, предоставляет собой инструменты для создания и тренировки нейронных сетей и других алгоритмов машинного обучения\n",
    "from tensorflow import keras #высокоуровневая API для создания и обучения нейронных сетей, интегрированая в TensorFlow.Используется для построения и тренировки моделей.\n",
    "from tensorflow.keras import layers #позволяет использовать различные слои, доступные в Keras\n",
    "import numpy as np #библиотека для работы с многомерными массивами и матрицами, а также для выполнения математических операций над ними,помогает обрабатывать и манипулировать данными\n",
    "import json # для загрузки вопросов из датасета\n",
    "import os #библиотека для взаимодействия с операционной системой\n",
    "\n",
    "\n",
    "def create_image_model(input_shape): # создание функции create_image_model, которая принимает один параметр — input_shape, он указывает, какого размера будут входные данные\n",
    "    inputs = layers.Input(shape=input_shape)# создание входного слоя, который будет принимать изображения в указанном формате\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu')(inputs) # обавление свёрточного слоя, который использует 32 фильтра размером 3x3,он помогает модели находить важные детали в изображениях\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x) # добавление слоя подвыборки, он уменьшает размер изображения в два раза, сохраняя только самое важное\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x) # свёрточный слой, с 64 фильтрами, этот слой ищет детали в изображении\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x) # слой подвыборки для дальнейшего уменьшения размера данных\n",
    "    x = layers.Flatten()(x) # преобразование многомерных данных (которые получились после всех свёрток и подвыборок) в одномерный вектор, для того чтобы передать данные в следующий слой\n",
    "    x = layers.Dense(128, activation='relu')(x) # добавление полносвязного слоя с 128 нейронами для того, чтобы модели могли делать более сложные выводы на основе извлеченных признаков\n",
    "    model = keras.Model(inputs, x) # создание модели, которая связывает входные данные с выходом для понимания, как будет выглядеть модель\n",
    "    return model # возвращение созданной модели, чтобы ее можно было использовать для обучения и предсказаний\n",
    "\n",
    "def create_text_model(vocab_size, embedding_dim, input_length): # функция create_text_model принимает три параметра:\n",
    "#vocab_size — количество уникальных слов в словаре\n",
    "#embedding_dim — размерность векторного представления каждого слова\n",
    "#input_length — длина входных текстов (количество слов в одном тексте)\n",
    "    \n",
    "    inputs = layers.Input(shape=(input_length,)) # создание входного слоя, который будет принимать последовательности слов фиксированной длины input_length\n",
    "    x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs) # слой встраивания (Embedding), он преобразует каждое слово в вектор фиксированной длины (embedding_dim)\n",
    "    # использует словарь размером vocab_size, чтобы сопоставить каждое слово с его векторным представлением\n",
    "    x = layers.LSTM(128)(x) # добавление слоя LSTM (долгая краткосрочная память) с 128 нейронами, он помогает модели обрабатывать последовательные данные, такие как текст, и запоминать контекст\n",
    "    model = keras.Model(inputs, x) # создание модели, которая связывает входные данные с выходом (вектором, полученным из LSTM)\n",
    "    return model # возвращение созданной модели, чтобы ее можно было использовать для обучения и предсказаний\n",
    "\n",
    "def create_combined_model(image_model, text_model):  # функция create_combined_model принимает два параметра:\n",
    "# image_model — модель для обработки изображений,\n",
    "# text_model — модель для обработки текстов\n",
    "    \n",
    "    combined_input = layers.concatenate([image_model.output, text_model.output]) # объединение выхода обеих моделей (изображений и текста) в один общий вектор, что позволяет модели использовать информацию из обеих источников одновременно\n",
    "    x = layers.Dense(64, activation='relu')(combined_input) # добавление полносвязного слоя с 64 нейронами и функцией активации ReLU, он помогает модели извлекать более сложные паттерны из объединенных данных\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)  # \n",
    "    model = keras.Model(inputs=[image_model.input, text_model.input], outputs=outputs) # Ссздание выходноого слоя с одним нейроном и функцией \n",
    "    # активации сигмоиды(математическая функция, которая используется в нейронных сетях для преобразования выходных значений нейронов в диапазон от 0 до 1), \n",
    "    # он будет использоваться для бинарной классификации, то есть для предсказания двух классов\n",
    "    return model# возвращение созданной модели, чтобы ее можно было использовать для обучения и предсказаний\n",
    "\n",
    "image_input_shape = (64, 64, 3)  # входных изображений, которые будут подаваться в модель, каждое изображение будет иметь размер 64 на 64 пикселя, и будет в цвете (RGB), что обозначается третьим значением 3\n",
    "vocab_size = 1000  # размер словаря, модель будет учитывать только 1000 уникальных слов\n",
    "embedding_dim = 64  # размерность векторного представления (эмбеддинга) для слов в тексте, каждое слово будет представлено в виде вектора размером 64\n",
    "max_question_length = 20  # максимальная длина вопросов, которые будут обрабатываться моделью, модель будет принимать только вопросы длиной до 20 слов\n",
    "\n",
    "\n",
    "image_model = create_image_model(image_input_shape) # создание модели для обработки изображений, используя заданную форму входных изображений image_input_shape, которая равна (64, 64, 3)\n",
    "# image_model будет представлять собой нейронную сеть, способную обрабатывать и анализировать изображения\n",
    "text_model = create_text_model(vocab_size, embedding_dim, max_question_length) # создание модели для обработки текста, используя параметры словаря, размерности эмбеддинга и максимальной длины вопросов\n",
    "# text_model будет представлять собой нейронную сеть, способную обрабатывать текстовые данные и извлекать из них смысл\n",
    "combined_model = create_combined_model(image_model, text_model) # создает комбинированную модель, которая объединяет как модель для обработки изображений, так и модель для обработки текста\n",
    "# combined_model будет представлять собой полную модель, способную одновременно обрабатывать как визуальные, так и текстовые данные\n",
    "\n",
    "\n",
    "combined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # compile подготавливает модель к обучению\n",
    "# определяет, как модель будет обновлять свои веса во время обучения и какие метрики будут использоваться для оценки её производительности\n",
    "# optimizer='adam' оптимизатор Adam адаптивно изменяет скорость обучения для каждого параметра\n",
    "# loss='binary_crossentropy' функция потерь, которая будет использоваться для оценки качества предсказаний модели\n",
    "# metrics=['accuracy'] метрики, которые будут отслеживаться во время обучения и оценки модели, accuracy измеряет долю правильных предсказаний модели по сравнению с общим числом предсказаний\n",
    "\n",
    "\n",
    "def load_clevr_questions(data_dir): # функция load_clevr_questions принимает аргумент data_dir, этот аргумент представляет собой путь к директории\n",
    "    questions = [] # пустой список questions, который будет использоваться для хранения загруженных вопросов из набора данных\n",
    "    answers = [] # пустой список answers, который будет использоваться для хранения ответов на вопросы\n",
    "\n",
    "    for split in ['CLEVR_train', 'CLEVR_val']: # цикл for, чтобы пройти по двум значениям: 'CLEVR_train' и 'CLEVR_val'\n",
    "        questions_file = os.path.join(data_dir, f'{split}_questions.json') # создается полное имя файла с вопросами, используя os.path.join, чтобы правильно объединить путь к директории data_dir\n",
    "        if not os.path.exists(questions_file): # проверяется существование файла с вопросами по указанному пути, если файл не найден, то условие not os.path.exists(questions_file) будет истинным\n",
    "            raise FileNotFoundError(f'Файл не найден: {questions_file}') # если файл не найден, будет вызвано исключение FileNotFoundError, и будет выведено сообщение о том, какой именно файл не найден\n",
    "\n",
    "        with open(questions_file, 'r') as f: #  with автоматически закроет файл после завершения блока кода, даже если возникнет ошибка\n",
    "            # open(questions_file, 'r') открывает файл, путь к которому хранится в переменной questions_file, в режиме чтения ('r')\n",
    "            # as f присваивает открытый файл переменной f, чтобы можно было ссылаться на него внутри блока with\n",
    "            question_data = json.load(f) # функция json.load() загружает содержимое файла f в формате JSON и преобразовать его в соответствующий объект Python\n",
    "             #question_data теперь будет содержать данные, которые были в файле questions_file\n",
    "\n",
    "        for item in question_data['questions']: # for проходится по каждому элементу в списке questions, который находится в загруженных данных question_data\n",
    "            # question_data —словарь, содержащий ключ 'questions', значение которого является списком, содержащим словари с вопросами и ответами\n",
    "            questions.append(item['question']) # внутри цикла идет обращение к текущему элементу item, который представляет собой словарь, содержащий информацию о вопросе и ответе\n",
    "            # item['question'] извлекает текст вопроса из текущего элемента, и добавляет его в список questions с помощью метода append(), это собирает все вопросы в одном списке\n",
    "            answers.append(item['answer']) # идет извлечение ответа из текущего элемента с помощью item['answer'] и добавление его в список answers\n",
    "            # после выполнения этого цикла будет два списка: один с вопросами и другой с соответствующими ответами\n",
    "\n",
    "    return questions, answers # возвращение двух списков: один с вопросами и другой с ответами для того, чтобы использовать эти списки в других частях программы\n",
    "\n",
    "\n",
    "CLEVR_DATA_DIR = 'C:/Users/Dasha/Desktop/CLEVR_v1.0' # путь к папке с данными\n",
    "\n",
    "try: # в блоке try программа выполняет функцию load_clevr_questions, которая загружает вопросы и ответы из указанной папки\n",
    "    questions, answers = load_clevr_questions(CLEVR_DATA_DIR) # в переменные questions и answers записываются загруженные данные\n",
    "    \n",
    "    print(f\"Количество вопросов: {len(questions)}\")\n",
    "    print(f\"Пример вопроса: {questions[0]}\")\n",
    "    print(f\"Пример ответа: {answers[0]}\")\n",
    "    \n",
    "except FileNotFoundError as e: # \n",
    "    print(e) # \n",
    "except json.JSONDecodeError: # \n",
    "    print(\"Ошибка при загрузке данных: файл не является корректным JSON.\") # \n",
    "except Exception as e: # \n",
    "    print(f\"Произошла ошибка: {e}\") # \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder # импортируется LabelEncoder из библиотеки sklearn, которая используется для обработки данных\n",
    "\n",
    "label_encoder = LabelEncoder() # создание объекта label_encoder, который будет использоваться для преобразования текстовых ответов в числовые значения\n",
    "encoded_answers = label_encoder.fit_transform(answers) # метод fit_transform для преобразования списка answers в числовой формат\n",
    "\n",
    "num_samples = 1000 # количество примеров, которые генерируются для тренировки модели\n",
    "x_image_train = np.random.rand(num_samples, *image_input_shape)  # генерируются случайные числа для изображений, которые будут использоваться для тренировки модели\n",
    "# используется функция np.random.rand из библиотеки NumPy, которая генерирует случайные числа в диапазоне от 0 до 1\n",
    "# num_samples - количество примеров, которое генерируется\n",
    "# *image_input_shape - это форма входного изображения\n",
    "x_text_train = np.random.randint(0, vocab_size, size=(num_samples, max_question_length))  # генерируются случайные целые числа для текстовых данных, которые будут использоваться для тренировки модели\n",
    "# используется функция np.random.randint из библиотеки NumPy, которая генерирует случайные целые числа в диапазоне от 0 до vocab_size\n",
    "# num_samples - количество примеров, которое генерируется\n",
    "# max_question_length - максимальная длина текстового вопроса\n",
    "# vocab_size - количество уникальных слов в словаре (vocabularly)\n",
    "y_train = np.random.randint(0, 2, size=(num_samples,))  # генерируются случайные метки (labels) для тренировки модели\n",
    "# используется функция np.random.randint из библиотеки NumPy, которая генерирует случайные целые числа в диапазоне от 0 до 1\n",
    "\n",
    "\n",
    "combined_model.fit([x_image_train, x_text_train], y_train, epochs=10, batch_size=32) \n",
    "# x_image_train и x_text_train - это входные данные для модели, они представляют собой изображения и текстовые данные\n",
    "# y_train - это метки (labels) для данных, которые были сгенерированы, они представляют собой классификацию данных (0 или 1)\n",
    "# epochs=10 - это количество эпох для обучения модели, одна эпоха - это один проход по всем данным\n",
    "# batch_size=32 - это количество примеров, которые обрабатываются за один шаг\n",
    "\n",
    "\n",
    "combined_model.summary() # информация о структуре модели combined_model для понимания, как работает модель и какие слои она содержит\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7",
   "language": "python",
   "name": "my_python_2_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
