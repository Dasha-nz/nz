{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dasha-nz/nz/blob/main/%D0%9A.%D0%9B./%D0%9B%D0%B0%D0%B1%D0%BE%D1%80%D0%B0%D1%82%D0%BE%D1%80%D0%BD%D0%B0%D1%8F_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy  # Импортируем библиотеку spaCy для обработки естественного языка\n",
        "import re  # Импортируем модуль re для работы с регулярными выражениями\n",
        "\n",
        "# Загружаем модель spaCy для английского языка (используйте \"ru_core_news_sm\" для русского)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Пример текста (электронного письма)\n",
        "email_text = \"\"\"\n",
        "Приветствую! Меня зовут Николай Сидоров. Если вам нужно со мной связаться, пишите\n",
        "на электронную почту nikolay.sidorov@example.com или можете обратиться к моей коллеге\n",
        "Светлане Федоровой (svetlana.fedorova@domain.com). Обращаю внимание, что я не всегда смогу ответить мгновенно.\n",
        "\"\"\"\n",
        "\n",
        "# Функция для маскировки имен и адресов электронной почты\n",
        "def mask_entities(text):\n",
        "    # Применяем модель к тексту, чтобы получить объекты Doc\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Маскируем имена\n",
        "    for ent in doc.ents:  # Проходим по всем распознанным сущностям в тексте\n",
        "        if ent.label_ == \"PERSON\":  # Проверяем, является ли сущность именем человека\n",
        "            text = text.replace(ent.text, \"[MASKED_PERSON]\")  # Заменяем имя на маску\n",
        "\n",
        "    # Маскируем адреса электронной почты с помощью регулярного выражения\n",
        "    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '[MASKED_EMAIL]', text)\n",
        "\n",
        "    return text  # Возвращаем текст с маскированными сущностями\n",
        "\n",
        "# Применяем функцию к тексту\n",
        "masked_text = mask_entities(email_text)  # Получаем текст с замаскированными именами и адресами\n",
        "print(masked_text)  # Выводим замаскированный текст на экран\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb_aBrkFXahs",
        "outputId": "05c4a5a7-4a59-4672-9285-e6a86b48ae6e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Приветствую! Меня зовут [MASKED_PERSON]. [MASKED_PERSON] вам нужно со мной связаться, [MASKED_PERSON]на электронную почту [MASKED_EMAIL] или можете обратиться к моей коллеге \n",
            "Светлане Федоровой ([MASKED_EMAIL]). Обращаю внимание, что я не всегда смогу ответить мгновенно.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter  # Импортируем класс Counter для подсчета частоты элементов в списке\n",
        "import re  # Импортируем модуль re для работы с регулярными выражениями\n",
        "\n",
        "# Пример описаний вакансий\n",
        "job_descriptions = [\n",
        "    \"Мы ищем талантливого Java-разработчика, обладающего крепкими знаниями в фреймворках Spring и Hibernate.\"\n",
        "    \"Кандидат должен иметь опыт работы с REST и SOAP API, а также уверенные навыки работы с реляционными базами данных.\"\n",
        "    \"Предпочтение будет отдаваться кандидатам с опытом разработки на Java 11, использованием Spring Boot и инструментов сборки, таких как Maven.\"\n",
        "    \"Важными требованиями являются умение писать чистый и поддерживаемый код, знание SQL и опыт работы в Agile-командах.\"\n",
        "    \"Понимание принципов микросервисной архитектуры и опыт работы с современными инструментами разработки будут являться дополнительными преимуществами.\"\n",
        "]\n",
        "\n",
        "# Регулярное выражение для поиска ключевых навыков в описаниях вакансий\n",
        "skills_pattern = r\"\\b(Java 11|Spring Boot|Hibernate|Maven|RESTful|SOAP API|SQL|архитектура микросервисов|Agile-среда)\\b\"\n",
        "\n",
        "# Ищем и считаем навыки\n",
        "skills = []  # Инициализируем пустой список для хранения найденных навыков\n",
        "for description in job_descriptions:  # Проходим по каждому описанию вакансии\n",
        "    found_skills = re.findall(skills_pattern, description)  # Находим все навыки, соответствующие регулярному выражению\n",
        "    skills.extend(found_skills)  # Добавляем найденные навыки в общий список\n",
        "\n",
        "# Получаем топ-5 навыков\n",
        "top_skills = Counter(skills).most_common(5)  # Подсчитываем частоту каждого навыка и получаем 5 самых распространенных\n",
        "print(\"Топ-5 требуемых навыков для Java-разработчика:\")  # Выводим заголовок\n",
        "for skill, count in top_skills:  # Проходим по списку топ-5 навыков\n",
        "    print(f\"{skill}: {count}\")  # Выводим каждый навык и его количество\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM_ssiCTX4mE",
        "outputId": "ef46cc01-020a-4056-f626-bc9a6d9ebbbf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Топ-5 требуемых навыков для Java-разработчика:\n",
            "Hibernate: 1\n",
            "SOAP API: 1\n",
            "Java 11: 1\n",
            "Spring Boot: 1\n",
            "Maven: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # Импортируем библиотеку NumPy для работы с массивами\n",
        "from keras.models import Sequential  # Импортируем класс Sequential для создания модели\n",
        "from keras.layers import LSTM, Dense, Embedding  # Импортируем необходимые слои для модели\n",
        "from keras.preprocessing.sequence import pad_sequences  # Импортируем функцию для паддинга последовательностей\n",
        "from sklearn.metrics import classification_report  # Импортируем для оценки качества классификации\n",
        "\n",
        "# Пример входных данных для обучения\n",
        "X_train = [[1, 2, 3], [4, 5, 6]]  # Пример входных данных (номера токенов)\n",
        "y_train = [[0, 1, 0], [1, 0, 1]]  # Пример целевых данных (метки)\n",
        "\n",
        "# Паддинг последовательностей для выравнивания их по длине\n",
        "X_train_padded = pad_sequences(X_train, padding='post')  # Добавляем нули в конец последовательностей для выравнивания\n",
        "\n",
        "# Определяем размер словаря и размерность эмбеддингов\n",
        "vocab_size = 10  # Количество уникальных токенов в словаре\n",
        "embedding_dim = 5  # Размерность вектора эмбеддинга\n",
        "\n",
        "# Создаем модель\n",
        "model = Sequential()  # Инициализируем последовательную модель\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=X_train_padded.shape[1]))  # Добавляем слой эмбеддинга\n",
        "model.add(LSTM(units=10, return_sequences=True))  # Добавляем слой LSTM с 10 единицами, возвращающим последовательности\n",
        "model.add(Dense(2, activation='softmax'))  # Добавляем полносвязный слой с 2 выходами и активацией softmax для многоклассовой классификации\n",
        "\n",
        "# Компилируем модель с оптимизатором Adam и функцией потерь sparse_categorical_crossentropy\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_padded, np.array(y_train), epochs=5)  # Обучаем модель на подготовленных данных\n",
        "\n",
        "# Тестовые данные\n",
        "X_test = [[1, 2, 3], [4, 5, 7]]  # Пример входных данных для тестирования\n",
        "y_test = [[0, 1, 0], [1, 0, 1]]  # Пример целевых данных для тестирования\n",
        "\n",
        "# Паддинг тестовых данных\n",
        "X_test_padded = pad_sequences(X_test, padding='post', maxlen=X_train_padded.shape[1])  # Паддинг тестовых данных с той же длиной, что и у обучающих\n",
        "\n",
        "# Прогнозирование\n",
        "y_pred_probs = model.predict(X_test_padded)  # Получаем вероятности предсказаний от модели\n",
        "y_pred = np.argmax(y_pred_probs, axis=-1)  # Получаем предсказанные метки, выбирая индекс с максимальной вероятностью\n",
        "\n",
        "# Оценка модели\n",
        "print(\"Предсказания:\", y_pred)  # Выводим предсказанные метки\n",
        "print(\"Истинные метки:\", y_test)  # Выводим истинные метки\n",
        "\n",
        "# Приводим y_test и y_pred к одномерному виду для сравнения\n",
        "y_test_flat = [label for sublist in y_test for label in sublist]  # Преобразуем y_test в одномерный список\n",
        "y_pred_flat = [label for sublist in y_pred for label in sublist]  # Преобразуем y_pred в одномерный список\n",
        "\n",
        "print(classification_report(y_test_flat, y_pred_flat))  # Выводим отчет о классификации для оценки результатов\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC5-dlOXYD9E",
        "outputId": "7279f670-62de-4d5d-f23d-546b8054bcf7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5000 - loss: 0.6933\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6930\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6928\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6667 - loss: 0.6926\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6667 - loss: 0.6924\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
            "Предсказания: [[0 0 0]\n",
            " [1 1 1]]\n",
            "Истинные метки: [[0, 1, 0], [1, 0, 1]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.67      0.67         3\n",
            "           1       0.67      0.67      0.67         3\n",
            "\n",
            "    accuracy                           0.67         6\n",
            "   macro avg       0.67      0.67      0.67         6\n",
            "weighted avg       0.67      0.67      0.67         6\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # Импортируем библиотеку NumPy для работы с массивами\n",
        "import pandas as pd  # Импортируем библиотеку Pandas для работы с данными (не используется в данном коде)\n",
        "from gensim.models import Word2Vec  # Импортируем модель Word2Vec из библиотеки Gensim для обучения векторных представлений слов\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Импортируем функцию для паддинга последовательностей\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Input  # Импортируем необходимые слои для модели\n",
        "from tensorflow.keras.models import Model  # Импортируем класс Model для создания модели\n",
        "from tensorflow.keras.utils import to_categorical  # Импортируем функцию для преобразования меток в категориальный формат\n",
        "from sklearn.model_selection import train_test_split  # Импортируем функцию для разделения данных на обучающую и тестовую выборки\n",
        "\n",
        "# Пример данных (токенизированные предложения)\n",
        "sentences = [\n",
        "    ['Иван', 'Петров', 'работает', 'в', 'компании', 'Яндекс', '.'],\n",
        "    ['Сергей', 'Сидоров', 'учится', 'в', 'Московском', 'государственном', 'университете', '.'],\n",
        "    ['Компания', 'Google', 'находится', 'в', 'Силиконовой', 'долине', '.'],\n",
        "    ['Москва', 'является', 'столицей', 'России', '.'],\n",
        "]\n",
        "\n",
        "# Метки для каждой сущности\n",
        "labels = [\n",
        "    ['B-PER', 'I-PER', 'O', 'O', 'O', 'B-ORG', 'O'],  # Иван Петров - PER (персона)\n",
        "    ['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O'],  # Сергей Сидоров - PER, МГУ - ORG\n",
        "    ['O', 'B-ORG', 'O', 'O', 'B-LOC', 'I-LOC', 'O'],  # Google - ORG, Силиконовая долина - LOC\n",
        "    ['B-LOC', 'O', 'O', 'O'],  # Москва - LOC, Россия - LOC\n",
        "]\n",
        "\n",
        "# Подготовка данных\n",
        "word_index = {word: i for i, word in enumerate(set(word for sentence in sentences for word in sentence), start=1)}\n",
        "# Создаем индекс слов, начиная с 1, чтобы 0 оставался для паддинга\n",
        "num_classes = len(set(label for sublist in labels for label in sublist)) + 1  # +1 для класса \"O\"\n",
        "max_len = max(len(sentence) for sentence in sentences)  # Определяем максимальную длину предложений\n",
        "\n",
        "# Преобразование слов в индексы и меток в числовые значения\n",
        "X = [[word_index[word] for word in sentence] for sentence in sentences]  # Преобразуем слова в индексы\n",
        "y = [[0 if label == 'O' else 1 for label in label_list] for label_list in labels]  # Преобразуем метки: O=0, B-entity=1\n",
        "\n",
        "# Паддинг последовательностей\n",
        "X_pad = pad_sequences(X, maxlen=max_len, padding='post')  # Паддинг входных данных\n",
        "y_pad = pad_sequences(y, maxlen=max_len, padding='post')  # Паддинг меток\n",
        "y_pad = to_categorical(y_pad, num_classes=num_classes)  # Преобразуем метки в категориальный формат\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_pad, test_size=0.2, random_state=42)  # Разделяем данные\n",
        "\n",
        "# Обучение модели Word2Vec\n",
        "model_w2v = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)  # Обучаем Word2Vec на предложениях\n",
        "\n",
        "# Создание матрицы эмбеддингов\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 100))  # Инициализируем матрицу эмбеддингов нулями\n",
        "for word, i in word_index.items():  # Заполняем матрицу эмбеддингов\n",
        "    if word in model_w2v.wv.key_to_index:\n",
        "        embedding_matrix[i] = model_w2v.wv[word]  # Заполняем вектором слова из модели Word2Vec\n",
        "\n",
        "# Построение модели LSTM-CRF\n",
        "input_layer = Input(shape=(max_len,))  # Входной слой\n",
        "embedding_layer = Embedding(input_dim=len(word_index) + 1,\n",
        "                             output_dim=100,\n",
        "                             weights=[embedding_matrix],\n",
        "                             trainable=False)(input_layer)  # Слой эмбеддинга с предобученной матрицей\n",
        "lstm_layer = LSTM(units=100, return_sequences=True)(embedding_layer)  # Слой LSTM\n",
        "output_layer = TimeDistributed(Dense(num_classes, activation='softmax'))(lstm_layer)  # Выходной слой с softmax для многоклассовой классификации\n",
        "\n",
        "# Создание и компиляция модели\n",
        "model = Model(inputs=input_layer, outputs=output_layer)  # Создаем модель\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Компилируем модель\n",
        "\n",
        "# Обучение модели\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10)  # Обучаем модель на обучающей выборке\n",
        "\n",
        "# Оценка производительности на тестовой выборке\n",
        "loss, accuracy = model.evaluate(X_test, y_test)  # Оцениваем модель на тестовой выборке\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')  # Выводим результаты тестирования\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMCkdD-OyTMo",
        "outputId": "9f7b6f52-eb8f-408e-94c0-6a933ca9413e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 2.0802\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7083 - loss: 2.0729\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7083 - loss: 2.0654\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7083 - loss: 2.0574\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7083 - loss: 2.0487\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7083 - loss: 2.0391\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7083 - loss: 2.0285\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7083 - loss: 2.0168\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7083 - loss: 2.0037\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7083 - loss: 1.9890\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.3750 - loss: 2.0096\n",
            "Test Loss: 2.009617805480957, Test Accuracy: 0.375\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}